{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "LFfpGrjucX_9",
      "metadata": {
        "id": "LFfpGrjucX_9"
      },
      "source": [
        "# Vision Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9qdKtHiXcgPR",
      "metadata": {
        "id": "9qdKtHiXcgPR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7507140",
      "metadata": {
        "id": "e7507140"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from timm.models import create_model\n",
        "\n",
        "from engine import train_one_epoch, train_one_epoch_distillation, evaluate\n",
        "from utils import get_training_dataloader, get_test_dataloader\n",
        "import models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BCwdDassciz1",
      "metadata": {
        "id": "BCwdDassciz1"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "In order to display the training statistics, I ran the evaluate function on the training dataset. Since the evaluate function already prints the top 1 accuracy and loss, I removed the print statements. I also needed to modify the model file because it didn't apply the num_classes argument we pass in when pretrained was false."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234c6ce8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "234c6ce8",
        "outputId": "19b5bbdb-f4b7-42e6-c4b4-83e73e534fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model: vit_base_patch16_224\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "number of params: 85806346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "CHECKPOINT_PATH = './checkpoint'\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "shots = 1000\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=False,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "cifar10_training_loader = get_training_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=2,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    shots=shots\n",
        ")\n",
        "\n",
        "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
        "\n",
        "cifar10_test_loader = get_test_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=4,\n",
        "    batch_size=256,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f51e2794",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f51e2794",
        "outputId": "4011ce67-0c11-4adf-c275-5d2e0c8397f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:17:00  loss: 2.3859 (2.3859)  time: 1.6321  data: 0.3980  max mem: 2092\n",
            "Epoch: [1]  [100/625]  eta: 0:01:46  loss: 2.1168 (2.3098)  time: 0.1674  data: 0.0053  max mem: 3055\n",
            "Epoch: [1]  [200/625]  eta: 0:01:19  loss: 2.0270 (2.1972)  time: 0.1697  data: 0.0052  max mem: 3055\n",
            "Epoch: [1]  [300/625]  eta: 0:00:59  loss: 2.0101 (2.1558)  time: 0.1753  data: 0.0069  max mem: 3055\n",
            "Epoch: [1]  [400/625]  eta: 0:00:41  loss: 2.0280 (2.1319)  time: 0.2072  data: 0.0115  max mem: 3055\n",
            "Epoch: [1]  [500/625]  eta: 0:00:22  loss: 2.0186 (2.1131)  time: 0.1790  data: 0.0059  max mem: 3055\n",
            "Epoch: [1]  [600/625]  eta: 0:00:04  loss: 2.0206 (2.0919)  time: 0.1742  data: 0.0051  max mem: 3055\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 1.8799 (2.0871)  time: 0.1810  data: 0.0098  max mem: 3055\n",
            "Epoch: [1] Total time: 0:01:53 (0.1818 s / it)\n",
            "Averaged stats: loss: 1.8799 (2.0871)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:05:43  loss: 1.8474 (1.8474)  acc1: 37.5000 (37.5000)  acc5: 87.5000 (87.5000)  time: 0.5489  data: 0.1940  max mem: 3055\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.9162 (1.9413)  acc1: 25.0000 (25.8100)  acc5: 87.5000 (82.1400)  time: 0.0591  data: 0.0053  max mem: 3055\n",
            "Test: Total time: 0:00:39 (0.0640 s / it)\n",
            "* Acc@1 25.810 Acc@5 82.140 loss 1.941\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:48  loss: 2.0047 (2.0047)  acc1: 22.2656 (22.2656)  acc5: 83.9844 (83.9844)  time: 4.2164  data: 3.0889  max mem: 3144\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 2.0242 (2.0317)  acc1: 23.8281 (24.7700)  acc5: 80.4688 (81.6100)  time: 0.8623  data: 0.0548  max mem: 3144\n",
            "Test: Total time: 0:00:40 (1.0135 s / it)\n",
            "* Acc@1 24.770 Acc@5 81.610 loss 2.032\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:53  loss: 1.8762 (1.8762)  time: 0.3737  data: 0.1856  max mem: 3144\n",
            "Epoch: [2]  [100/625]  eta: 0:01:36  loss: 1.8606 (1.9490)  time: 0.1740  data: 0.0050  max mem: 3144\n",
            "Epoch: [2]  [200/625]  eta: 0:01:16  loss: 1.8190 (1.9447)  time: 0.1787  data: 0.0076  max mem: 3144\n",
            "Epoch: [2]  [300/625]  eta: 0:00:58  loss: 1.8518 (1.9419)  time: 0.1811  data: 0.0104  max mem: 3144\n",
            "Epoch: [2]  [400/625]  eta: 0:00:40  loss: 1.7893 (1.9171)  time: 0.1737  data: 0.0048  max mem: 3144\n",
            "Epoch: [2]  [500/625]  eta: 0:00:22  loss: 1.7641 (1.9015)  time: 0.1736  data: 0.0048  max mem: 3144\n",
            "Epoch: [2]  [600/625]  eta: 0:00:04  loss: 1.8149 (1.8936)  time: 0.1790  data: 0.0080  max mem: 3144\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 1.9645 (1.8939)  time: 0.1785  data: 0.0080  max mem: 3144\n",
            "Epoch: [2] Total time: 0:01:51 (0.1780 s / it)\n",
            "Averaged stats: loss: 1.9645 (1.8939)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:00  loss: 1.8203 (1.8203)  acc1: 50.0000 (50.0000)  acc5: 68.7500 (68.7500)  time: 0.1920  data: 0.1387  max mem: 3144\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.8047 (1.8388)  acc1: 25.0000 (30.4000)  acc5: 87.5000 (84.4900)  time: 0.0678  data: 0.0130  max mem: 3144\n",
            "Test: Total time: 0:00:39 (0.0627 s / it)\n",
            "* Acc@1 30.400 Acc@5 84.490 loss 1.839\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:40  loss: 1.8397 (1.8397)  acc1: 32.4219 (32.4219)  acc5: 84.3750 (84.3750)  time: 4.0202  data: 2.9535  max mem: 3144\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.8640 (1.8604)  acc1: 30.8594 (30.4500)  acc5: 82.0312 (83.1200)  time: 0.8630  data: 0.0574  max mem: 3144\n",
            "Test: Total time: 0:00:39 (0.9929 s / it)\n",
            "* Acc@1 30.450 Acc@5 83.120 loss 1.860\n",
            "Epoch: [3]  [  0/625]  eta: 0:03:29  loss: 1.5630 (1.5630)  time: 0.3352  data: 0.1429  max mem: 3144\n",
            "Epoch: [3]  [100/625]  eta: 0:01:33  loss: 1.8852 (1.8780)  time: 0.1747  data: 0.0057  max mem: 3144\n",
            "Epoch: [3]  [200/625]  eta: 0:01:15  loss: 1.8304 (1.8559)  time: 0.1832  data: 0.0108  max mem: 3144\n",
            "Epoch: [3]  [300/625]  eta: 0:00:57  loss: 1.8313 (1.8342)  time: 0.1753  data: 0.0062  max mem: 3144\n",
            "Epoch: [3]  [400/625]  eta: 0:00:39  loss: 1.7459 (1.8286)  time: 0.1740  data: 0.0052  max mem: 3144\n",
            "Epoch: [3]  [500/625]  eta: 0:00:22  loss: 1.7700 (1.8231)  time: 0.1746  data: 0.0054  max mem: 3144\n",
            "Epoch: [3]  [600/625]  eta: 0:00:04  loss: 1.8357 (1.8177)  time: 0.1827  data: 0.0105  max mem: 3144\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 1.7410 (1.8181)  time: 0.1751  data: 0.0057  max mem: 3144\n",
            "Epoch: [3] Total time: 0:01:50 (0.1775 s / it)\n",
            "Averaged stats: loss: 1.7410 (1.8181)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:09  loss: 1.7624 (1.7624)  acc1: 25.0000 (25.0000)  acc5: 93.7500 (93.7500)  time: 0.2065  data: 0.1483  max mem: 3144\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.7354 (1.7888)  acc1: 31.2500 (31.2600)  acc5: 87.5000 (86.0400)  time: 0.0707  data: 0.0141  max mem: 3144\n",
            "Test: Total time: 0:00:39 (0.0633 s / it)\n",
            "* Acc@1 31.260 Acc@5 86.040 loss 1.789\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:02  loss: 1.7863 (1.7863)  acc1: 30.8594 (30.8594)  acc5: 88.2812 (88.2812)  time: 3.0614  data: 2.0684  max mem: 3147\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.8171 (1.8057)  acc1: 31.6406 (31.5800)  acc5: 84.3750 (85.3500)  time: 0.8784  data: 0.0656  max mem: 3367\n",
            "Test: Total time: 0:00:38 (0.9671 s / it)\n",
            "* Acc@1 31.580 Acc@5 85.350 loss 1.806\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:18  loss: 1.9263 (1.9263)  time: 0.3182  data: 0.1314  max mem: 3367\n",
            "Epoch: [4]  [100/625]  eta: 0:01:33  loss: 1.8020 (1.8066)  time: 0.1781  data: 0.0076  max mem: 3367\n",
            "Epoch: [4]  [200/625]  eta: 0:01:15  loss: 1.7273 (1.7827)  time: 0.1779  data: 0.0072  max mem: 3367\n",
            "Epoch: [4]  [300/625]  eta: 0:00:57  loss: 1.7226 (1.7777)  time: 0.1737  data: 0.0052  max mem: 3367\n",
            "Epoch: [4]  [400/625]  eta: 0:00:39  loss: 1.7915 (1.7793)  time: 0.1738  data: 0.0050  max mem: 3367\n",
            "Epoch: [4]  [500/625]  eta: 0:00:22  loss: 1.7060 (1.7762)  time: 0.1828  data: 0.0111  max mem: 3367\n",
            "Epoch: [4]  [600/625]  eta: 0:00:04  loss: 1.8125 (1.7767)  time: 0.1829  data: 0.0104  max mem: 3367\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 1.7352 (1.7761)  time: 0.1783  data: 0.0076  max mem: 3367\n",
            "Epoch: [4] Total time: 0:01:51 (0.1777 s / it)\n",
            "Averaged stats: loss: 1.7352 (1.7761)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:07  loss: 1.4809 (1.4809)  acc1: 50.0000 (50.0000)  acc5: 87.5000 (87.5000)  time: 0.2037  data: 0.1510  max mem: 3367\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.7965 (1.7713)  acc1: 31.2500 (34.3800)  acc5: 87.5000 (86.0700)  time: 0.0720  data: 0.0163  max mem: 3367\n",
            "Test: Total time: 0:00:39 (0.0633 s / it)\n",
            "* Acc@1 34.380 Acc@5 86.070 loss 1.771\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:15  loss: 1.7970 (1.7970)  acc1: 30.4688 (30.4688)  acc5: 85.5469 (85.5469)  time: 3.3996  data: 2.3258  max mem: 3367\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.7578 (1.7572)  acc1: 34.7656 (34.2300)  acc5: 86.3281 (86.0300)  time: 0.8687  data: 0.0595  max mem: 3367\n",
            "Test: Total time: 0:00:38 (0.9660 s / it)\n",
            "* Acc@1 34.230 Acc@5 86.030 loss 1.757\n",
            "Epoch: [5]  [  0/625]  eta: 0:03:29  loss: 2.2291 (2.2291)  time: 0.3350  data: 0.1354  max mem: 3367\n",
            "Epoch: [5]  [100/625]  eta: 0:01:33  loss: 1.6941 (1.7190)  time: 0.1789  data: 0.0081  max mem: 3367\n",
            "Epoch: [5]  [200/625]  eta: 0:01:15  loss: 1.6226 (1.7085)  time: 0.1817  data: 0.0096  max mem: 3367\n",
            "Epoch: [5]  [300/625]  eta: 0:00:57  loss: 1.6158 (1.7159)  time: 0.1735  data: 0.0049  max mem: 3367\n",
            "Epoch: [5]  [400/625]  eta: 0:00:39  loss: 1.6468 (1.7159)  time: 0.1736  data: 0.0049  max mem: 3367\n",
            "Epoch: [5]  [500/625]  eta: 0:00:22  loss: 1.7645 (1.7220)  time: 0.1777  data: 0.0080  max mem: 3367\n",
            "Epoch: [5]  [600/625]  eta: 0:00:04  loss: 1.7162 (1.7263)  time: 0.1807  data: 0.0092  max mem: 3367\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 1.7732 (1.7287)  time: 0.1738  data: 0.0050  max mem: 3367\n",
            "Epoch: [5] Total time: 0:01:50 (0.1770 s / it)\n",
            "Averaged stats: loss: 1.7732 (1.7287)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:06  loss: 1.8920 (1.8920)  acc1: 25.0000 (25.0000)  acc5: 81.2500 (81.2500)  time: 0.2032  data: 0.1377  max mem: 3367\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.6544 (1.6977)  acc1: 37.5000 (37.2100)  acc5: 93.7500 (88.2600)  time: 0.0592  data: 0.0049  max mem: 3367\n",
            "Test: Total time: 0:00:40 (0.0640 s / it)\n",
            "* Acc@1 37.210 Acc@5 88.260 loss 1.698\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:48  loss: 1.6802 (1.6802)  acc1: 39.4531 (39.4531)  acc5: 88.2812 (88.2812)  time: 2.7022  data: 1.7420  max mem: 3367\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.7428 (1.7195)  acc1: 34.7656 (34.7900)  acc5: 87.8906 (87.4900)  time: 0.8695  data: 0.0613  max mem: 3367\n",
            "Test: Total time: 0:00:38 (0.9510 s / it)\n",
            "* Acc@1 34.790 Acc@5 87.490 loss 1.719\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "    print(\"TRAINING STATISTICS\")\n",
        "    training_stats = evaluate(cifar10_training_loader, model, criterion, device)\n",
        "    print(\"TEST STATISTICS\")\n",
        "    test_stats = evaluate(cifar10_test_loader, model, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e323d040",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e323d040",
        "outputId": "4bf14110-8b54-406d-9802-e37ea0144833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:02:09  loss: 1.6802 (1.6802)  acc1: 39.4531 (39.4531)  acc5: 88.2812 (88.2812)  time: 3.2348  data: 2.2521  max mem: 3367\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.7428 (1.7195)  acc1: 34.7656 (34.7900)  acc5: 87.8906 (87.4900)  time: 0.8659  data: 0.0563  max mem: 3367\n",
            "Test: Total time: 0:00:38 (0.9591 s / it)\n",
            "* Acc@1 34.790 Acc@5 87.490 loss 1.719\n",
            "Throughput: 260.63119392691556\n"
          ]
        }
      ],
      "source": [
        "# Calculate througput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yxPCcdIfei9q",
      "metadata": {
        "id": "yxPCcdIfei9q"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "This model (finetuned base model) is the teacher that we'll be using later in the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caaf4f8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caaf4f8a",
        "outputId": "c7925e1a-bc77-4548-f998-ed52534df5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model: vit_base_patch16_224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 167MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of params: 85806346\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Train the teacher\n",
        "\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "teacher = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "teacher = teacher.to(device)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c392f94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c392f94",
        "outputId": "60cd83cd-c7d3-4d94-becf-f087581ddc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:16:01  loss: 2.6647 (2.6647)  time: 1.5387  data: 0.5186  max mem: 2420\n",
            "Epoch: [1]  [100/625]  eta: 0:01:36  loss: 2.0907 (2.2181)  time: 0.1678  data: 0.0051  max mem: 3379\n",
            "Epoch: [1]  [200/625]  eta: 0:01:16  loss: 1.7393 (2.0804)  time: 0.1772  data: 0.0096  max mem: 3379\n",
            "Epoch: [1]  [300/625]  eta: 0:00:58  loss: 1.3831 (1.9155)  time: 0.1730  data: 0.0066  max mem: 3379\n",
            "Epoch: [1]  [400/625]  eta: 0:00:40  loss: 1.2427 (1.7650)  time: 0.1707  data: 0.0049  max mem: 3379\n",
            "Epoch: [1]  [500/625]  eta: 0:00:22  loss: 1.0401 (1.6468)  time: 0.1729  data: 0.0048  max mem: 3379\n",
            "Epoch: [1]  [600/625]  eta: 0:00:04  loss: 0.8741 (1.5349)  time: 0.1817  data: 0.0086  max mem: 3379\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 0.8170 (1.5098)  time: 0.1778  data: 0.0056  max mem: 3379\n",
            "Epoch: [1] Total time: 0:01:51 (0.1779 s / it)\n",
            "Averaged stats: loss: 0.8170 (1.5098)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:04:27  loss: 0.6830 (0.6830)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.4279  data: 0.1616  max mem: 3379\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.7898 (0.8610)  acc1: 75.0000 (70.1200)  acc5: 100.0000 (97.8600)  time: 0.0704  data: 0.0160  max mem: 3379\n",
            "Test: Total time: 0:00:40 (0.0650 s / it)\n",
            "* Acc@1 70.120 Acc@5 97.860 loss 0.861\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:40  loss: 1.0423 (1.0423)  acc1: 65.2344 (65.2344)  acc5: 96.4844 (96.4844)  time: 4.0073  data: 2.9935  max mem: 3472\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.9759 (0.9423)  acc1: 65.6250 (66.8500)  acc5: 97.6562 (97.3800)  time: 0.8864  data: 0.0771  max mem: 3472\n",
            "Test: Total time: 0:00:39 (0.9862 s / it)\n",
            "* Acc@1 66.850 Acc@5 97.380 loss 0.942\n",
            "Epoch: [2]  [  0/625]  eta: 0:04:07  loss: 0.6057 (0.6057)  time: 0.3955  data: 0.1673  max mem: 3472\n",
            "Epoch: [2]  [100/625]  eta: 0:01:43  loss: 0.7614 (0.8737)  time: 0.1770  data: 0.0072  max mem: 3472\n",
            "Epoch: [2]  [200/625]  eta: 0:01:20  loss: 0.6751 (0.8351)  time: 0.1835  data: 0.0109  max mem: 3472\n",
            "Epoch: [2]  [300/625]  eta: 0:01:00  loss: 0.6794 (0.8087)  time: 0.1749  data: 0.0053  max mem: 3472\n",
            "Epoch: [2]  [400/625]  eta: 0:00:41  loss: 0.6690 (0.7896)  time: 0.1762  data: 0.0051  max mem: 3472\n",
            "Epoch: [2]  [500/625]  eta: 0:00:22  loss: 0.6504 (0.7608)  time: 0.1882  data: 0.0099  max mem: 3472\n",
            "Epoch: [2]  [600/625]  eta: 0:00:04  loss: 0.6539 (0.7458)  time: 0.1810  data: 0.0070  max mem: 3472\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 0.7052 (0.7436)  time: 0.1781  data: 0.0054  max mem: 3472\n",
            "Epoch: [2] Total time: 0:01:54 (0.1834 s / it)\n",
            "Averaged stats: loss: 0.7052 (0.7436)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:16  loss: 0.4495 (0.4495)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.2184  data: 0.1626  max mem: 3472\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.5076 (0.4930)  acc1: 81.2500 (83.4300)  acc5: 100.0000 (99.5800)  time: 0.0598  data: 0.0051  max mem: 3472\n",
            "Test: Total time: 0:00:40 (0.0646 s / it)\n",
            "* Acc@1 83.430 Acc@5 99.580 loss 0.493\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:04  loss: 0.6709 (0.6709)  acc1: 75.7812 (75.7812)  acc5: 99.6094 (99.6094)  time: 3.1149  data: 2.1590  max mem: 3472\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.6341 (0.6086)  acc1: 77.7344 (78.5200)  acc5: 99.2188 (98.9900)  time: 0.8941  data: 0.0708  max mem: 3472\n",
            "Test: Total time: 0:00:39 (0.9787 s / it)\n",
            "* Acc@1 78.520 Acc@5 98.990 loss 0.609\n",
            "Epoch: [3]  [  0/625]  eta: 0:05:13  loss: 0.6218 (0.6218)  time: 0.5014  data: 0.3034  max mem: 3472\n",
            "Epoch: [3]  [100/625]  eta: 0:01:36  loss: 0.5739 (0.5460)  time: 0.1808  data: 0.0074  max mem: 3472\n",
            "Epoch: [3]  [200/625]  eta: 0:01:17  loss: 0.3237 (0.5237)  time: 0.1765  data: 0.0054  max mem: 3472\n",
            "Epoch: [3]  [300/625]  eta: 0:00:58  loss: 0.4558 (0.5141)  time: 0.1766  data: 0.0052  max mem: 3472\n",
            "Epoch: [3]  [400/625]  eta: 0:00:40  loss: 0.4314 (0.5218)  time: 0.1892  data: 0.0126  max mem: 3472\n",
            "Epoch: [3]  [500/625]  eta: 0:00:22  loss: 0.4619 (0.5179)  time: 0.1779  data: 0.0058  max mem: 3472\n",
            "Epoch: [3]  [600/625]  eta: 0:00:04  loss: 0.4701 (0.5162)  time: 0.1783  data: 0.0057  max mem: 3472\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 0.4167 (0.5150)  time: 0.1804  data: 0.0078  max mem: 3472\n",
            "Epoch: [3] Total time: 0:01:53 (0.1811 s / it)\n",
            "Averaged stats: loss: 0.4167 (0.5150)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:03:23  loss: 0.2323 (0.2323)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.3259  data: 0.2661  max mem: 3472\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.3102 (0.3427)  acc1: 87.5000 (88.2800)  acc5: 100.0000 (99.6700)  time: 0.0598  data: 0.0047  max mem: 3472\n",
            "Test: Total time: 0:00:39 (0.0638 s / it)\n",
            "* Acc@1 88.280 Acc@5 99.670 loss 0.343\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:28  loss: 0.5427 (0.5427)  acc1: 80.8594 (80.8594)  acc5: 99.2188 (99.2188)  time: 3.7224  data: 2.6555  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.5139 (0.5147)  acc1: 81.6406 (82.2200)  acc5: 99.2188 (99.2100)  time: 0.8641  data: 0.0533  max mem: 4284\n",
            "Test: Total time: 0:00:39 (0.9860 s / it)\n",
            "* Acc@1 82.220 Acc@5 99.210 loss 0.515\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:27  loss: 0.2395 (0.2395)  time: 0.3324  data: 0.1517  max mem: 4284\n",
            "Epoch: [4]  [100/625]  eta: 0:01:34  loss: 0.2922 (0.3545)  time: 0.1775  data: 0.0055  max mem: 4284\n",
            "Epoch: [4]  [200/625]  eta: 0:01:16  loss: 0.4791 (0.3766)  time: 0.1838  data: 0.0086  max mem: 4284\n",
            "Epoch: [4]  [300/625]  eta: 0:00:58  loss: 0.3073 (0.3859)  time: 0.1874  data: 0.0117  max mem: 4284\n",
            "Epoch: [4]  [400/625]  eta: 0:00:40  loss: 0.3133 (0.3887)  time: 0.1771  data: 0.0054  max mem: 4284\n",
            "Epoch: [4]  [500/625]  eta: 0:00:22  loss: 0.4498 (0.3957)  time: 0.1775  data: 0.0055  max mem: 4284\n",
            "Epoch: [4]  [600/625]  eta: 0:00:04  loss: 0.4462 (0.3948)  time: 0.1877  data: 0.0111  max mem: 4284\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.3411 (0.3956)  time: 0.1772  data: 0.0050  max mem: 4284\n",
            "Epoch: [4] Total time: 0:01:52 (0.1805 s / it)\n",
            "Averaged stats: loss: 0.3411 (0.3956)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:25  loss: 0.3045 (0.3045)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.2323  data: 0.1786  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.1605 (0.2482)  acc1: 93.7500 (91.4200)  acc5: 100.0000 (99.9100)  time: 0.0694  data: 0.0109  max mem: 4284\n",
            "Test: Total time: 0:00:40 (0.0650 s / it)\n",
            "* Acc@1 91.420 Acc@5 99.910 loss 0.248\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:06  loss: 0.4719 (0.4719)  acc1: 83.5938 (83.5938)  acc5: 98.4375 (98.4375)  time: 3.1513  data: 2.2007  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.4942 (0.4881)  acc1: 82.0312 (83.2100)  acc5: 100.0000 (99.5700)  time: 0.8801  data: 0.0595  max mem: 4284\n",
            "Test: Total time: 0:00:38 (0.9727 s / it)\n",
            "* Acc@1 83.210 Acc@5 99.570 loss 0.488\n",
            "Epoch: [5]  [  0/625]  eta: 0:03:34  loss: 0.1381 (0.1381)  time: 0.3430  data: 0.1570  max mem: 4284\n",
            "Epoch: [5]  [100/625]  eta: 0:01:35  loss: 0.3274 (0.3138)  time: 0.1864  data: 0.0090  max mem: 4284\n",
            "Epoch: [5]  [200/625]  eta: 0:01:16  loss: 0.1967 (0.3007)  time: 0.1767  data: 0.0054  max mem: 4284\n",
            "Epoch: [5]  [300/625]  eta: 0:00:58  loss: 0.3028 (0.3050)  time: 0.1766  data: 0.0055  max mem: 4284\n",
            "Epoch: [5]  [400/625]  eta: 0:00:40  loss: 0.2345 (0.2933)  time: 0.1768  data: 0.0054  max mem: 4284\n",
            "Epoch: [5]  [500/625]  eta: 0:00:22  loss: 0.3225 (0.2942)  time: 0.1873  data: 0.0119  max mem: 4284\n",
            "Epoch: [5]  [600/625]  eta: 0:00:04  loss: 0.3170 (0.2976)  time: 0.1771  data: 0.0053  max mem: 4284\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.3038 (0.2990)  time: 0.1775  data: 0.0056  max mem: 4284\n",
            "Epoch: [5] Total time: 0:01:52 (0.1797 s / it)\n",
            "Averaged stats: loss: 0.3038 (0.2990)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:03:19  loss: 0.0735 (0.0735)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.3196  data: 0.2436  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.2082 (0.2244)  acc1: 93.7500 (92.3100)  acc5: 100.0000 (99.9000)  time: 0.0604  data: 0.0053  max mem: 4284\n",
            "Test: Total time: 0:00:41 (0.0657 s / it)\n",
            "* Acc@1 92.310 Acc@5 99.900 loss 0.224\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:59  loss: 0.5970 (0.5970)  acc1: 80.0781 (80.0781)  acc5: 98.8281 (98.8281)  time: 2.9874  data: 1.9912  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.5428 (0.5722)  acc1: 82.4219 (82.5700)  acc5: 99.2188 (99.2800)  time: 0.8851  data: 0.0671  max mem: 4284\n",
            "Test: Total time: 0:00:38 (0.9665 s / it)\n",
            "* Acc@1 82.570 Acc@5 99.280 loss 0.572\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        teacher, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "    print(\"TRAINING STATISTICS\")\n",
        "    training_stats = evaluate(cifar10_training_loader, teacher, criterion, device)\n",
        "    print(\"TEST STATISTICS\")\n",
        "    test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab509fd",
      "metadata": {
        "id": "9ab509fd"
      },
      "outputs": [],
      "source": [
        "# save finetuned teacher model\n",
        "torch.save(teacher.state_dict(), './teacher.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13D7xxoaNtEr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13D7xxoaNtEr",
        "outputId": "fb6994ff-fb6e-4137-cea9-3ca09d159203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:02:03  loss: 0.5970 (0.5970)  acc1: 80.0781 (80.0781)  acc5: 98.8281 (98.8281)  time: 3.0875  data: 2.0882  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.5428 (0.5722)  acc1: 82.4219 (82.5700)  acc5: 99.2188 (99.2800)  time: 0.8880  data: 0.0627  max mem: 4284\n",
            "Test: Total time: 0:00:39 (0.9950 s / it)\n",
            "* Acc@1 82.570 Acc@5 99.280 loss 0.572\n"
          ]
        }
      ],
      "source": [
        "teacher = create_model(\n",
        "        'vit_base_patch16_224',\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "teacher = teacher.to(device)\n",
        "teacher.load_state_dict(torch.load('./teacher.pth'))\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d9evO_1nLgA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d9evO_1nLgA",
        "outputId": "d8f2a43a-86b0-4b2b-e6d1-e39da269dba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:02:33  loss: 0.5970 (0.5970)  acc1: 80.0781 (80.0781)  acc5: 98.8281 (98.8281)  time: 3.8428  data: 2.8354  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.5428 (0.5722)  acc1: 82.4219 (82.5700)  acc5: 99.2188 (99.2800)  time: 0.8813  data: 0.0632  max mem: 4284\n",
            "Test: Total time: 0:00:39 (0.9883 s / it)\n",
            "* Acc@1 82.570 Acc@5 99.280 loss 0.572\n",
            "Throughput: 252.89932980445153\n"
          ]
        }
      ],
      "source": [
        "# Calculate throughput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8_DoVz3Je-7L",
      "metadata": {
        "id": "8_DoVz3Je-7L"
      },
      "source": [
        "Pretraining on ImageNet produces significantly better results than training from scratch on CIFAR-10. This is because ImageNet provides us some good initial features to work off of, rather than our model needing to learn it all on its own. This is the essence of transfer learning, which works if we don't have as many resources or enough data and want to use a bigger model to help improve our own model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arQWw5X4fTxU",
      "metadata": {
        "id": "arQWw5X4fTxU"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Let's finetune the vit_tiny model first as a baseline before we try knowledge distillation in the next problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ku8mP6ShQ1U0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku8mP6ShQ1U0",
        "outputId": "8648b21d-b4e3-4f5e-d837-db0f5b408317"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to /root/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth\n",
            "100%|██████████| 21.9M/21.9M [00:00<00:00, 80.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of params: 5526346\n",
            "Start training for 5 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1]  [  0/625]  eta: 0:03:00  loss: 2.6602 (2.6602)  time: 0.2880  data: 0.1580  max mem: 4284\n",
            "Epoch: [1]  [100/625]  eta: 0:00:39  loss: 2.1288 (2.2799)  time: 0.0558  data: 0.0049  max mem: 4284\n",
            "Epoch: [1]  [200/625]  eta: 0:00:28  loss: 2.0735 (2.1918)  time: 0.0557  data: 0.0046  max mem: 4284\n",
            "Epoch: [1]  [300/625]  eta: 0:00:22  loss: 1.9165 (2.1087)  time: 0.0804  data: 0.0087  max mem: 4284\n",
            "Epoch: [1]  [400/625]  eta: 0:00:14  loss: 1.6505 (2.0478)  time: 0.0587  data: 0.0050  max mem: 4284\n",
            "Epoch: [1]  [500/625]  eta: 0:00:08  loss: 1.7300 (1.9995)  time: 0.0927  data: 0.0146  max mem: 4284\n",
            "Epoch: [1]  [600/625]  eta: 0:00:01  loss: 1.6376 (1.9572)  time: 0.0573  data: 0.0048  max mem: 4284\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 1.7289 (1.9464)  time: 0.0542  data: 0.0045  max mem: 4284\n",
            "Epoch: [1] Total time: 0:00:40 (0.0656 s / it)\n",
            "Averaged stats: loss: 1.7289 (1.9464)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:11  loss: 2.1653 (2.1653)  acc1: 6.2500 (6.2500)  acc5: 75.0000 (75.0000)  time: 0.2101  data: 0.1603  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.6404 (1.6777)  acc1: 37.5000 (37.5500)  acc5: 81.2500 (88.1500)  time: 0.0355  data: 0.0097  max mem: 4284\n",
            "Test: Total time: 0:00:27 (0.0445 s / it)\n",
            "* Acc@1 37.550 Acc@5 88.150 loss 1.678\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:33  loss: 1.7438 (1.7438)  acc1: 40.2344 (40.2344)  acc5: 85.1562 (85.1562)  time: 2.3417  data: 1.9617  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.7076 (1.6937)  acc1: 37.1094 (37.5400)  acc5: 86.7188 (86.5300)  time: 0.3751  data: 0.1241  max mem: 4284\n",
            "Test: Total time: 0:00:19 (0.4994 s / it)\n",
            "* Acc@1 37.540 Acc@5 86.530 loss 1.694\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:17  loss: 2.0618 (2.0618)  time: 0.3167  data: 0.1856  max mem: 4284\n",
            "Epoch: [2]  [100/625]  eta: 0:00:49  loss: 1.7807 (1.6589)  time: 0.0576  data: 0.0053  max mem: 4284\n",
            "Epoch: [2]  [200/625]  eta: 0:00:32  loss: 1.5509 (1.6018)  time: 0.0572  data: 0.0048  max mem: 4284\n",
            "Epoch: [2]  [300/625]  eta: 0:00:24  loss: 1.3261 (1.5504)  time: 0.0579  data: 0.0049  max mem: 4284\n",
            "Epoch: [2]  [400/625]  eta: 0:00:15  loss: 1.3423 (1.5064)  time: 0.0597  data: 0.0050  max mem: 4284\n",
            "Epoch: [2]  [500/625]  eta: 0:00:08  loss: 1.4377 (1.4841)  time: 0.0744  data: 0.0067  max mem: 4284\n",
            "Epoch: [2]  [600/625]  eta: 0:00:01  loss: 1.1736 (1.4467)  time: 0.0581  data: 0.0049  max mem: 4284\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 1.0437 (1.4361)  time: 0.0546  data: 0.0047  max mem: 4284\n",
            "Epoch: [2] Total time: 0:00:43 (0.0691 s / it)\n",
            "Averaged stats: loss: 1.0437 (1.4361)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:16  loss: 1.3856 (1.3856)  acc1: 68.7500 (68.7500)  acc5: 93.7500 (93.7500)  time: 0.2190  data: 0.1715  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.1009 (1.1498)  acc1: 56.2500 (59.0100)  acc5: 93.7500 (95.9300)  time: 0.0360  data: 0.0137  max mem: 4284\n",
            "Test: Total time: 0:00:27 (0.0443 s / it)\n",
            "* Acc@1 59.010 Acc@5 95.930 loss 1.150\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:41  loss: 1.2862 (1.2862)  acc1: 54.2969 (54.2969)  acc5: 95.3125 (95.3125)  time: 2.5436  data: 2.1839  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.2515 (1.2296)  acc1: 53.9062 (55.8100)  acc5: 95.3125 (95.1900)  time: 0.3850  data: 0.1312  max mem: 4284\n",
            "Test: Total time: 0:00:20 (0.5095 s / it)\n",
            "* Acc@1 55.810 Acc@5 95.190 loss 1.230\n",
            "Epoch: [3]  [  0/625]  eta: 0:04:01  loss: 0.9996 (0.9996)  time: 0.3867  data: 0.2464  max mem: 4284\n",
            "Epoch: [3]  [100/625]  eta: 0:00:36  loss: 1.1064 (1.1720)  time: 0.0586  data: 0.0048  max mem: 4284\n",
            "Epoch: [3]  [200/625]  eta: 0:00:26  loss: 1.0883 (1.1292)  time: 0.0605  data: 0.0051  max mem: 4284\n",
            "Epoch: [3]  [300/625]  eta: 0:00:21  loss: 1.0437 (1.1181)  time: 0.0575  data: 0.0050  max mem: 4284\n",
            "Epoch: [3]  [400/625]  eta: 0:00:14  loss: 1.1008 (1.0996)  time: 0.0574  data: 0.0048  max mem: 4284\n",
            "Epoch: [3]  [500/625]  eta: 0:00:08  loss: 1.0320 (1.0854)  time: 0.0585  data: 0.0049  max mem: 4284\n",
            "Epoch: [3]  [600/625]  eta: 0:00:01  loss: 1.0389 (1.0693)  time: 0.0576  data: 0.0049  max mem: 4284\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 1.0107 (1.0696)  time: 0.0549  data: 0.0046  max mem: 4284\n",
            "Epoch: [3] Total time: 0:00:40 (0.0643 s / it)\n",
            "Averaged stats: loss: 1.0107 (1.0696)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:18  loss: 0.6941 (0.6941)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.2220  data: 0.1703  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.8728 (0.8987)  acc1: 62.5000 (67.6500)  acc5: 100.0000 (97.8700)  time: 0.0353  data: 0.0126  max mem: 4284\n",
            "Test: Total time: 0:00:27 (0.0447 s / it)\n",
            "* Acc@1 67.650 Acc@5 97.870 loss 0.899\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:52  loss: 1.0336 (1.0336)  acc1: 63.2812 (63.2812)  acc5: 96.0938 (96.0938)  time: 2.8115  data: 2.3997  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.9883 (0.9983)  acc1: 63.2812 (64.5400)  acc5: 96.8750 (96.9500)  time: 0.4277  data: 0.1696  max mem: 4284\n",
            "Test: Total time: 0:00:21 (0.5351 s / it)\n",
            "* Acc@1 64.540 Acc@5 96.950 loss 0.998\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:47  loss: 0.9618 (0.9618)  time: 0.3640  data: 0.2279  max mem: 4284\n",
            "Epoch: [4]  [100/625]  eta: 0:00:32  loss: 0.7504 (0.8603)  time: 0.0579  data: 0.0052  max mem: 4284\n",
            "Epoch: [4]  [200/625]  eta: 0:00:27  loss: 0.6898 (0.8488)  time: 0.0925  data: 0.0124  max mem: 4284\n",
            "Epoch: [4]  [300/625]  eta: 0:00:21  loss: 0.9144 (0.8640)  time: 0.0585  data: 0.0051  max mem: 4284\n",
            "Epoch: [4]  [400/625]  eta: 0:00:14  loss: 0.8422 (0.8637)  time: 0.0766  data: 0.0090  max mem: 4284\n",
            "Epoch: [4]  [500/625]  eta: 0:00:08  loss: 0.7816 (0.8626)  time: 0.0581  data: 0.0050  max mem: 4284\n",
            "Epoch: [4]  [600/625]  eta: 0:00:01  loss: 0.6416 (0.8583)  time: 0.0571  data: 0.0047  max mem: 4284\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.7865 (0.8570)  time: 0.0788  data: 0.0096  max mem: 4284\n",
            "Epoch: [4] Total time: 0:00:40 (0.0649 s / it)\n",
            "Averaged stats: loss: 0.7865 (0.8570)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:03:47  loss: 0.6021 (0.6021)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 0.3644  data: 0.2751  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.7015 (0.7996)  acc1: 68.7500 (71.3000)  acc5: 100.0000 (98.7400)  time: 0.0347  data: 0.0124  max mem: 4284\n",
            "Test: Total time: 0:00:26 (0.0420 s / it)\n",
            "* Acc@1 71.300 Acc@5 98.740 loss 0.800\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:51  loss: 1.1060 (1.1060)  acc1: 64.0625 (64.0625)  acc5: 94.9219 (94.9219)  time: 2.7839  data: 2.4064  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.9674 (0.9957)  acc1: 65.2344 (65.0800)  acc5: 97.6562 (97.0300)  time: 0.4566  data: 0.1701  max mem: 4284\n",
            "Test: Total time: 0:00:23 (0.5788 s / it)\n",
            "* Acc@1 65.080 Acc@5 97.030 loss 0.996\n",
            "Epoch: [5]  [  0/625]  eta: 0:02:43  loss: 0.8247 (0.8247)  time: 0.2621  data: 0.1763  max mem: 4284\n",
            "Epoch: [5]  [100/625]  eta: 0:00:30  loss: 0.6152 (0.7058)  time: 0.0563  data: 0.0049  max mem: 4284\n",
            "Epoch: [5]  [200/625]  eta: 0:00:27  loss: 0.5836 (0.7057)  time: 0.0611  data: 0.0058  max mem: 4284\n",
            "Epoch: [5]  [300/625]  eta: 0:00:19  loss: 0.6907 (0.7100)  time: 0.0560  data: 0.0049  max mem: 4284\n",
            "Epoch: [5]  [400/625]  eta: 0:00:14  loss: 0.7210 (0.7068)  time: 0.0894  data: 0.0106  max mem: 4284\n",
            "Epoch: [5]  [500/625]  eta: 0:00:07  loss: 0.7644 (0.7005)  time: 0.0541  data: 0.0045  max mem: 4284\n",
            "Epoch: [5]  [600/625]  eta: 0:00:01  loss: 0.7697 (0.7083)  time: 0.0608  data: 0.0060  max mem: 4284\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.7248 (0.7082)  time: 0.0817  data: 0.0131  max mem: 4284\n",
            "Epoch: [5] Total time: 0:00:38 (0.0620 s / it)\n",
            "Averaged stats: loss: 0.7248 (0.7082)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:51  loss: 0.8761 (0.8761)  acc1: 68.7500 (68.7500)  acc5: 100.0000 (100.0000)  time: 0.2737  data: 0.2223  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.5574 (0.5785)  acc1: 81.2500 (79.8200)  acc5: 100.0000 (99.1700)  time: 0.0342  data: 0.0099  max mem: 4284\n",
            "Test: Total time: 0:00:25 (0.0407 s / it)\n",
            "* Acc@1 79.820 Acc@5 99.170 loss 0.578\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:56  loss: 0.9032 (0.9032)  acc1: 71.4844 (71.4844)  acc5: 96.8750 (96.8750)  time: 2.9067  data: 2.4853  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.8065 (0.8204)  acc1: 71.4844 (71.3000)  acc5: 98.4375 (98.2400)  time: 0.4133  data: 0.1422  max mem: 4284\n",
            "Test: Total time: 0:00:20 (0.5172 s / it)\n",
            "* Acc@1 71.300 Acc@5 98.240 loss 0.820\n"
          ]
        }
      ],
      "source": [
        "# Train the tiny model\n",
        "MODEL_NAME = 'vit_tiny_patch16_224'\n",
        "\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n",
        "\n",
        "\n",
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "\n",
        "    print(\"TRAINING STATISTICS\")\n",
        "    training_stats = evaluate(cifar10_training_loader, model, criterion, device)\n",
        "    print(\"TEST STATISTICS\")\n",
        "    test_stats = evaluate(cifar10_test_loader, model, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SMUB3nT9Y-RF",
      "metadata": {
        "id": "SMUB3nT9Y-RF"
      },
      "outputs": [],
      "source": [
        "# save finetuned tiny model\n",
        "torch.save(model.state_dict(), './tiny.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0LWoMONMb6uQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LWoMONMb6uQ",
        "outputId": "c17a1791-167f-4ed5-fb61-7c05819d3757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:01:38  loss: 0.9032 (0.9032)  acc1: 71.4844 (71.4844)  acc5: 96.8750 (96.8750)  time: 2.4577  data: 2.1075  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.8065 (0.8204)  acc1: 71.4844 (71.3000)  acc5: 98.4375 (98.2400)  time: 0.4410  data: 0.1676  max mem: 4284\n",
            "Test: Total time: 0:00:19 (0.4860 s / it)\n",
            "* Acc@1 71.300 Acc@5 98.240 loss 0.820\n"
          ]
        }
      ],
      "source": [
        "tiny = create_model(\n",
        "        'vit_tiny_patch16_224',\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "tiny = tiny.to(device)\n",
        "tiny.load_state_dict(torch.load('./tiny.pth'))\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, tiny, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7uOVErvzgLGg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uOVErvzgLGg",
        "outputId": "83466cdc-fc5b-47b3-e10a-b65c9c437441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:01:32  loss: 0.9032 (0.9032)  acc1: 71.4844 (71.4844)  acc5: 96.8750 (96.8750)  time: 2.3009  data: 1.9487  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.8065 (0.8204)  acc1: 71.4844 (71.3000)  acc5: 98.4375 (98.2400)  time: 0.3628  data: 0.1148  max mem: 4284\n",
            "Test: Total time: 0:00:19 (0.4864 s / it)\n",
            "* Acc@1 71.300 Acc@5 98.240 loss 0.820\n",
            "Throughput: 513.8633316699122\n"
          ]
        }
      ],
      "source": [
        "# Calculate throughput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, tiny, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-sgeMS-xgSmK",
      "metadata": {
        "id": "-sgeMS-xgSmK"
      },
      "source": [
        "The accuracy on our pretrained vit_tiny model is lower than the pretrained vit_base, as expected. However, it still performs much better than the vit_base model trained from scratch, showing the value of transfer learning. The throughput is higher on the tiny model because it's smaller, and the data goes through less processing to produce an output. So, the tiny model can process more data at a faster rate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYYfSiBrma_4",
      "metadata": {
        "id": "EYYfSiBrma_4"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Now, we can perform knowledge distillation and compare it to the pretrained vit_tiny model. We expect to see an improvement. I implemented knowledge distillation by first softening the logits with softmax with temperature (and log on the student logits) before two losses that make up the total loss. Then, I computed KL loss between the student and teacher probabilities multiplied by the temperature squared (which is generally suggested). I also computed the Cross-Entropy loss between the student logits and the targets. These are combined for the total loss, which is the sum of alpha times the KL loss and beta times the Cross-Entropy loss. Alpha and beta are hyperparameters, which I chose 0.3 and 0.7 for, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69587aa8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69587aa8",
        "outputId": "ff74ab46-75fd-43fd-b0d6-c36fb9574dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of params: 5526346\n",
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:03:44  loss: 2.6307 (2.6307)  time: 0.3592  data: 0.1519  max mem: 4284\n",
            "Epoch: [1]  [100/625]  eta: 0:00:57  loss: 2.0912 (2.1580)  time: 0.0997  data: 0.0053  max mem: 4284\n",
            "Epoch: [1]  [200/625]  eta: 0:00:45  loss: 1.8580 (2.0526)  time: 0.1004  data: 0.0051  max mem: 4284\n",
            "Epoch: [1]  [300/625]  eta: 0:00:34  loss: 1.8292 (1.9928)  time: 0.1151  data: 0.0099  max mem: 4284\n",
            "Epoch: [1]  [400/625]  eta: 0:00:23  loss: 1.6827 (1.9295)  time: 0.0983  data: 0.0046  max mem: 4284\n",
            "Epoch: [1]  [500/625]  eta: 0:00:13  loss: 1.5522 (1.8825)  time: 0.0984  data: 0.0049  max mem: 4284\n",
            "Epoch: [1]  [600/625]  eta: 0:00:02  loss: 1.4754 (1.8387)  time: 0.1021  data: 0.0059  max mem: 4284\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 1.6401 (1.8307)  time: 0.0976  data: 0.0045  max mem: 4284\n",
            "Epoch: [1] Total time: 0:01:05 (0.1052 s / it)\n",
            "Averaged stats: loss: 1.6401 (1.8307)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:01:56  loss: 1.6109 (1.6109)  acc1: 31.2500 (31.2500)  acc5: 100.0000 (100.0000)  time: 0.1862  data: 0.1573  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.5059 (1.5738)  acc1: 50.0000 (41.5600)  acc5: 93.7500 (91.2100)  time: 0.0328  data: 0.0076  max mem: 4284\n",
            "Test: Total time: 0:00:26 (0.0428 s / it)\n",
            "* Acc@1 41.560 Acc@5 91.210 loss 1.574\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:42  loss: 1.6446 (1.6446)  acc1: 39.8438 (39.8438)  acc5: 89.4531 (89.4531)  time: 2.5599  data: 2.1991  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.6189 (1.6160)  acc1: 39.8438 (39.6700)  acc5: 90.2344 (90.2500)  time: 0.3796  data: 0.1280  max mem: 4284\n",
            "Test: Total time: 0:00:19 (0.4843 s / it)\n",
            "* Acc@1 39.670 Acc@5 90.250 loss 1.616\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:08  loss: 1.6092 (1.6092)  time: 0.3012  data: 0.1553  max mem: 4284\n",
            "Epoch: [2]  [100/625]  eta: 0:01:03  loss: 1.3994 (1.4669)  time: 0.1103  data: 0.0066  max mem: 4284\n",
            "Epoch: [2]  [200/625]  eta: 0:00:48  loss: 1.4173 (1.4475)  time: 0.1203  data: 0.0094  max mem: 4284\n",
            "Epoch: [2]  [300/625]  eta: 0:00:35  loss: 1.4389 (1.4101)  time: 0.1040  data: 0.0052  max mem: 4284\n",
            "Epoch: [2]  [400/625]  eta: 0:00:24  loss: 1.1665 (1.3657)  time: 0.1001  data: 0.0053  max mem: 4284\n",
            "Epoch: [2]  [500/625]  eta: 0:00:13  loss: 1.0249 (1.3217)  time: 0.0993  data: 0.0049  max mem: 4284\n",
            "Epoch: [2]  [600/625]  eta: 0:00:02  loss: 1.0063 (1.2950)  time: 0.1247  data: 0.0110  max mem: 4284\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 1.0510 (1.2888)  time: 0.0980  data: 0.0050  max mem: 4284\n",
            "Epoch: [2] Total time: 0:01:07 (0.1085 s / it)\n",
            "Averaged stats: loss: 1.0510 (1.2888)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:17  loss: 0.7948 (0.7948)  acc1: 68.7500 (68.7500)  acc5: 93.7500 (93.7500)  time: 0.2200  data: 0.1712  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 1.1097 (1.0821)  acc1: 56.2500 (61.7700)  acc5: 100.0000 (96.3400)  time: 0.0361  data: 0.0091  max mem: 4284\n",
            "Test: Total time: 0:00:27 (0.0436 s / it)\n",
            "* Acc@1 61.770 Acc@5 96.340 loss 1.082\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:40  loss: 1.2488 (1.2488)  acc1: 56.6406 (56.6406)  acc5: 93.7500 (93.7500)  time: 2.5050  data: 2.1449  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.1600 (1.1565)  acc1: 59.3750 (58.9000)  acc5: 96.4844 (95.9000)  time: 0.4199  data: 0.1621  max mem: 4284\n",
            "Test: Total time: 0:00:19 (0.4888 s / it)\n",
            "* Acc@1 58.900 Acc@5 95.900 loss 1.156\n",
            "Epoch: [3]  [  0/625]  eta: 0:02:54  loss: 1.3593 (1.3593)  time: 0.2788  data: 0.1659  max mem: 4284\n",
            "Epoch: [3]  [100/625]  eta: 0:00:58  loss: 0.8749 (1.0028)  time: 0.1025  data: 0.0051  max mem: 4284\n",
            "Epoch: [3]  [200/625]  eta: 0:00:46  loss: 0.9334 (0.9775)  time: 0.1223  data: 0.0127  max mem: 4284\n",
            "Epoch: [3]  [300/625]  eta: 0:00:34  loss: 0.9533 (0.9893)  time: 0.0998  data: 0.0050  max mem: 4284\n",
            "Epoch: [3]  [400/625]  eta: 0:00:24  loss: 0.9525 (0.9848)  time: 0.1004  data: 0.0051  max mem: 4284\n",
            "Epoch: [3]  [500/625]  eta: 0:00:13  loss: 0.6990 (0.9520)  time: 0.0997  data: 0.0050  max mem: 4284\n",
            "Epoch: [3]  [600/625]  eta: 0:00:02  loss: 0.8883 (0.9324)  time: 0.1217  data: 0.0103  max mem: 4284\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 0.7853 (0.9266)  time: 0.1061  data: 0.0064  max mem: 4284\n",
            "Epoch: [3] Total time: 0:01:06 (0.1071 s / it)\n",
            "Averaged stats: loss: 0.7853 (0.9266)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:21  loss: 0.5155 (0.5155)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 0.2260  data: 0.1770  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.6662 (0.7696)  acc1: 75.0000 (72.8300)  acc5: 100.0000 (98.4500)  time: 0.0592  data: 0.0135  max mem: 4284\n",
            "Test: Total time: 0:00:26 (0.0432 s / it)\n",
            "* Acc@1 72.830 Acc@5 98.450 loss 0.770\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:53  loss: 0.9829 (0.9829)  acc1: 67.5781 (67.5781)  acc5: 97.2656 (97.2656)  time: 2.8251  data: 2.4841  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.9586 (0.9400)  acc1: 64.8438 (65.2100)  acc5: 98.0469 (97.4600)  time: 0.4407  data: 0.1687  max mem: 4284\n",
            "Test: Total time: 0:00:19 (0.4941 s / it)\n",
            "* Acc@1 65.210 Acc@5 97.460 loss 0.940\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:11  loss: 0.7052 (0.7052)  time: 0.3064  data: 0.1583  max mem: 4284\n",
            "Epoch: [4]  [100/625]  eta: 0:00:57  loss: 0.6673 (0.8010)  time: 0.1114  data: 0.0083  max mem: 4284\n",
            "Epoch: [4]  [200/625]  eta: 0:00:44  loss: 0.6456 (0.7641)  time: 0.1112  data: 0.0082  max mem: 4284\n",
            "Epoch: [4]  [300/625]  eta: 0:00:34  loss: 0.6280 (0.7510)  time: 0.0980  data: 0.0049  max mem: 4284\n",
            "Epoch: [4]  [400/625]  eta: 0:00:24  loss: 0.6512 (0.7408)  time: 0.0979  data: 0.0047  max mem: 4284\n",
            "Epoch: [4]  [500/625]  eta: 0:00:13  loss: 0.6660 (0.7318)  time: 0.1152  data: 0.0097  max mem: 4284\n",
            "Epoch: [4]  [600/625]  eta: 0:00:02  loss: 0.6069 (0.7255)  time: 0.0989  data: 0.0054  max mem: 4284\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.6838 (0.7234)  time: 0.1092  data: 0.0083  max mem: 4284\n",
            "Epoch: [4] Total time: 0:01:06 (0.1062 s / it)\n",
            "Averaged stats: loss: 0.6838 (0.7234)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:03:15  loss: 0.7582 (0.7582)  acc1: 68.7500 (68.7500)  acc5: 100.0000 (100.0000)  time: 0.3124  data: 0.2498  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.6959 (0.6831)  acc1: 75.0000 (75.3800)  acc5: 100.0000 (98.7200)  time: 0.0340  data: 0.0116  max mem: 4284\n",
            "Test: Total time: 0:00:26 (0.0419 s / it)\n",
            "* Acc@1 75.380 Acc@5 98.720 loss 0.683\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:02:08  loss: 0.9142 (0.9142)  acc1: 67.1875 (67.1875)  acc5: 95.7031 (95.7031)  time: 3.2137  data: 2.7819  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.8939 (0.8844)  acc1: 67.5781 (69.0500)  acc5: 97.6562 (97.7700)  time: 0.4403  data: 0.1596  max mem: 4284\n",
            "Test: Total time: 0:00:21 (0.5305 s / it)\n",
            "* Acc@1 69.050 Acc@5 97.770 loss 0.884\n",
            "Epoch: [5]  [  0/625]  eta: 0:02:57  loss: 0.4999 (0.4999)  time: 0.2834  data: 0.1433  max mem: 4284\n",
            "Epoch: [5]  [100/625]  eta: 0:00:55  loss: 0.6716 (0.6266)  time: 0.1093  data: 0.0081  max mem: 4284\n",
            "Epoch: [5]  [200/625]  eta: 0:00:44  loss: 0.6137 (0.6317)  time: 0.1019  data: 0.0049  max mem: 4284\n",
            "Epoch: [5]  [300/625]  eta: 0:00:34  loss: 0.6192 (0.6206)  time: 0.1002  data: 0.0055  max mem: 4284\n",
            "Epoch: [5]  [400/625]  eta: 0:00:24  loss: 0.5762 (0.6260)  time: 0.1165  data: 0.0090  max mem: 4284\n",
            "Epoch: [5]  [500/625]  eta: 0:00:13  loss: 0.6846 (0.6218)  time: 0.1065  data: 0.0063  max mem: 4284\n",
            "Epoch: [5]  [600/625]  eta: 0:00:02  loss: 0.4973 (0.6117)  time: 0.1001  data: 0.0048  max mem: 4284\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.5945 (0.6132)  time: 0.0995  data: 0.0048  max mem: 4284\n",
            "Epoch: [5] Total time: 0:01:06 (0.1059 s / it)\n",
            "Averaged stats: loss: 0.5945 (0.6132)\n",
            "TRAINING STATISTICS\n",
            "Test:  [  0/625]  eta: 0:02:30  loss: 0.3327 (0.3327)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.2414  data: 0.1639  max mem: 4284\n",
            "Test:  [624/625]  eta: 0:00:00  loss: 0.5125 (0.5107)  acc1: 81.2500 (82.9800)  acc5: 100.0000 (99.3700)  time: 0.0358  data: 0.0121  max mem: 4284\n",
            "Test: Total time: 0:00:27 (0.0443 s / it)\n",
            "* Acc@1 82.980 Acc@5 99.370 loss 0.511\n",
            "TEST STATISTICS\n",
            "Test:  [ 0/40]  eta: 0:01:57  loss: 0.8084 (0.8084)  acc1: 71.8750 (71.8750)  acc5: 98.0469 (98.0469)  time: 2.9346  data: 2.4787  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.7412 (0.7383)  acc1: 74.6094 (74.0900)  acc5: 98.8281 (98.5800)  time: 0.4363  data: 0.1645  max mem: 4284\n",
            "Test: Total time: 0:00:21 (0.5296 s / it)\n",
            "* Acc@1 74.090 Acc@5 98.580 loss 0.738\n"
          ]
        }
      ],
      "source": [
        "# Train the distilled student\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "MODEL_NAME = 'vit_tiny_patch16_224'\n",
        "\n",
        "distilled = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "distilled = distilled.to(device)\n",
        "\n",
        "optimizer = optim.Adam(distilled.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in distilled.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n",
        "\n",
        "\n",
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch_distillation(\n",
        "        teacher, distilled, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch, alpha=0.3, temp=1.0)\n",
        "\n",
        "    print(\"TRAINING STATISTICS\")\n",
        "    training_stats = evaluate(cifar10_training_loader, distilled, criterion, device)\n",
        "    print(\"TEST STATISTICS\")\n",
        "    test_stats = evaluate(cifar10_test_loader, distilled, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jg99fOTKdDaa",
      "metadata": {
        "id": "Jg99fOTKdDaa"
      },
      "outputs": [],
      "source": [
        "# save distilled student model\n",
        "torch.save(distilled.state_dict(), './distilled.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LsHGP4kcmHzQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsHGP4kcmHzQ",
        "outputId": "8e08f63a-e29f-483c-f238-bd550cdfec01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:01:36  loss: 0.8084 (0.8084)  acc1: 71.8750 (71.8750)  acc5: 98.0469 (98.0469)  time: 2.4157  data: 2.0551  max mem: 4284\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.7412 (0.7383)  acc1: 74.6094 (74.0900)  acc5: 98.8281 (98.5800)  time: 0.4288  data: 0.1700  max mem: 4284\n",
            "Test: Total time: 0:00:19 (0.4958 s / it)\n",
            "* Acc@1 74.090 Acc@5 98.580 loss 0.738\n"
          ]
        }
      ],
      "source": [
        "distilled = create_model(\n",
        "        'vit_tiny_patch16_224',\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "distilled = distilled.to(device)\n",
        "distilled.load_state_dict(torch.load('./distilled.pth'))\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, distilled, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qldmg-9MmYsh",
      "metadata": {
        "id": "Qldmg-9MmYsh"
      },
      "source": [
        "Knowledge distillation did improve our model, but only by a little bit. With this, we could now run a better tiny model on something like a phone. I believe that tuning the alpha and temperature of the loss function could improve the results, if we're willing to run the model many times."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
