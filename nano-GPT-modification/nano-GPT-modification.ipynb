{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TGUCQ7D-xXi"
   },
   "source": [
    "# Nano GPT Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHmGZ4kEDLrf"
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vwti1U94-vIn",
    "outputId": "105125d7-9d91-47bc-cc6e-225647b2b7f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "%run data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9itka4MJnb5b"
   },
   "source": [
    "Now, let's train the model. I'll be running a smaller model, but still for 5000 iterations. In this model, the dimensionality of the keys, values, and queries is $256/4=64$. Also, when evaluating models, I'm going to look through and pick the lowest validation loss because there's no early stopping on this model. We'd probably want to stop wherever validation loss is lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RTZINlLIDJaA",
    "outputId": "e861a644-887b-4f77-9e21-725b29775a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "wind = -1\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "n_kqv = -1\n",
      "dropout = 0.2\n",
      "big_mlp = False\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 100\n",
      "Overriding: block_size = 128\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 256\n",
      "Overriding: max_iters = 5000\n",
      "Overriding: lr_decay_iters = 5000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 3.16M\n",
      "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
      "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.1819, val loss 4.1759\n",
      "iter 0: loss 4.2032, time 1782.56ms, mfu -100.00%\n",
      "iter 100: loss 2.4970, time 22.13ms, mfu 0.46%\n",
      "iter 200: loss 2.3984, time 0.00ms, mfu 10.53%\n",
      "step 250: train loss 2.3695, val loss 2.3920\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 300: loss 2.2694, time 0.00ms, mfu 19.60%\n",
      "iter 400: loss 2.0653, time 15.63ms, mfu 17.71%\n",
      "step 500: train loss 1.9758, val loss 2.0718\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.0426, time 1906.59ms, mfu 15.94%\n",
      "iter 600: loss 2.0221, time 0.00ms, mfu 24.46%\n",
      "iter 700: loss 1.9235, time 15.63ms, mfu 22.08%\n",
      "step 750: train loss 1.8086, val loss 1.9432\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 800: loss 1.8234, time 15.63ms, mfu 19.94%\n",
      "iter 900: loss 1.7917, time 0.00ms, mfu 28.07%\n",
      "step 1000: train loss 1.6944, val loss 1.8489\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.7083, time 1953.08ms, mfu 25.26%\n",
      "iter 1100: loss 1.6122, time 0.00ms, mfu 32.86%\n",
      "iter 1200: loss 1.6792, time 22.13ms, mfu 29.62%\n",
      "step 1250: train loss 1.6126, val loss 1.7767\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1300: loss 1.5044, time 15.63ms, mfu 26.72%\n",
      "iter 1400: loss 1.6402, time 22.13ms, mfu 24.09%\n",
      "step 1500: train loss 1.5598, val loss 1.7432\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.5186, time 1933.14ms, mfu 21.68%\n",
      "iter 1600: loss 1.5488, time 0.00ms, mfu 29.64%\n",
      "iter 1700: loss 1.6371, time 15.63ms, mfu 26.74%\n",
      "step 1750: train loss 1.5110, val loss 1.7129\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1800: loss 1.4523, time 15.63ms, mfu 24.13%\n",
      "iter 1900: loss 1.4219, time 15.63ms, mfu 21.78%\n",
      "step 2000: train loss 1.4743, val loss 1.6850\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.4967, time 1927.70ms, mfu 19.60%\n",
      "iter 2100: loss 1.4151, time 15.63ms, mfu 17.71%\n",
      "iter 2200: loss 1.4050, time 0.00ms, mfu 26.06%\n",
      "step 2250: train loss 1.4439, val loss 1.6425\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2300: loss 1.4421, time 15.63ms, mfu 23.52%\n",
      "iter 2400: loss 1.4417, time 15.63ms, mfu 21.23%\n",
      "step 2500: train loss 1.4135, val loss 1.6328\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.4233, time 1910.26ms, mfu 19.11%\n",
      "iter 2600: loss 1.4291, time 15.64ms, mfu 17.26%\n",
      "iter 2700: loss 1.4058, time 0.00ms, mfu 25.66%\n",
      "step 2750: train loss 1.3815, val loss 1.6201\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2800: loss 1.4072, time 15.63ms, mfu 23.16%\n",
      "iter 2900: loss 1.3397, time 0.00ms, mfu 30.96%\n",
      "step 3000: train loss 1.3617, val loss 1.5770\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.3095, time 1933.15ms, mfu 27.87%\n",
      "iter 3100: loss 1.2901, time 0.00ms, mfu 35.20%\n",
      "iter 3200: loss 1.2886, time 6.23ms, mfu 31.84%\n",
      "step 3250: train loss 1.3400, val loss 1.5620\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3300: loss 1.4039, time 0.00ms, mfu 38.78%\n",
      "iter 3400: loss 1.3215, time 0.00ms, mfu 45.02%\n",
      "step 3500: train loss 1.3151, val loss 1.5545\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.3521, time 1923.96ms, mfu 40.52%\n",
      "iter 3600: loss 1.3116, time 0.00ms, mfu 46.59%\n",
      "iter 3700: loss 1.2669, time 15.33ms, mfu 42.00%\n",
      "step 3750: train loss 1.2966, val loss 1.5471\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3800: loss 1.3394, time 15.64ms, mfu 37.86%\n",
      "iter 3900: loss 1.2976, time 0.00ms, mfu 44.20%\n",
      "step 4000: train loss 1.2750, val loss 1.5237\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.2485, time 1966.92ms, mfu 39.78%\n",
      "iter 4100: loss 1.2799, time 26.69ms, mfu 35.84%\n",
      "iter 4200: loss 1.2458, time 0.00ms, mfu 42.38%\n",
      "step 4250: train loss 1.2581, val loss 1.5187\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4300: loss 1.2111, time 31.25ms, mfu 38.17%\n",
      "iter 4400: loss 1.2492, time 15.63ms, mfu 34.42%\n",
      "step 4500: train loss 1.2462, val loss 1.5291\n",
      "iter 4500: loss 1.2270, time 1863.55ms, mfu 30.98%\n",
      "iter 4600: loss 1.2058, time 31.24ms, mfu 27.91%\n",
      "iter 4700: loss 1.2603, time 15.63ms, mfu 25.19%\n",
      "step 4750: train loss 1.2359, val loss 1.5231\n",
      "iter 4800: loss 1.2457, time 15.63ms, mfu 22.73%\n",
      "iter 4900: loss 1.2369, time 15.63ms, mfu 20.52%\n",
      "step 5000: train loss 1.2258, val loss 1.5214\n",
      "iter 5000: loss 1.2267, time 1892.80ms, mfu 18.47%\n"
     ]
    }
   ],
   "source": [
    "%run train.py config/train_shakespeare_char.py --device=cuda --compile=False --eval_iters=200 --log_interval=100 --block_size=128 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=256 --max_iters=5000 --lr_decay_iters=5000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTulyMWkZfJ9"
   },
   "source": [
    "We can see that the final validation loss is 1.5214."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrdnvDQjtL4q"
   },
   "source": [
    "Here is a small sample of what this model generates. We see that it looks like it follows the pattern of Shakespeare's writing, and creates real words. However, when reading it, we see that the story makes no sense. It's just stringing phrases together. This is probably because we don't have a long enough context length, and also because we're using characters as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGZasP5cgQTF",
    "outputId": "6942f7ed-f191-453d-b8f5-a1a5350be751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "number of parameters: 3.16M\n",
      "Loading meta from data\\shakespeare_char\\meta.pkl...\n",
      "\n",
      "\n",
      "KING RICHARD II:\n",
      "Shall I, be made to be execution?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "My hard will I beseech you a frawn to you,\n",
      "Do not him the own of of it.\n",
      "\n",
      "DUKE OF YORK:\n",
      "When is the neceed?\n",
      "With clook the deed blood with your pelproof,\n",
      "That like in childishonourish her years in sun.\n",
      "\n",
      "SICINIUS:\n",
      "Indeedly, demong all alived with pieces with\n",
      "For rivers and men to-night.\n",
      "\n",
      "Third Citizen:\n",
      "Why, who resolve me to thy fleet to fight.\n",
      "\n",
      "CORIOLANUS:\n",
      "I see a time is a point of him for hithers.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "And for th\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%run small_sample.py --out_dir=out-shakespeare-char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pDJvxAfg2RV"
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fJ-EkoxsOvA"
   },
   "source": [
    "For this part, I used the linear layer to scale down the dimensionality of the keys, queries, and values. It takes in vectors of the size of the embeddings, and outputs vectors that are the desired size. Instead of having the full embedding size, the network learns fewer, but more general encodings of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedCausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_kqv * config.n_head, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_kqv * config.n_head, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_kqv = config.n_kqv\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_kqv * self.n_head, dim=2)\n",
    "        k = k.view(B, T, self.n_head, self.n_kqv).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, self.n_kqv).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, self.n_kqv).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, self.n_kqv * self.n_head) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QASyP6ZFthE8"
   },
   "source": [
    "Let's run the model with the dimensionality of our keys, queries, and values set to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TLi315Tg46A",
    "outputId": "728bebff-a04e-43c2-9433-7129bd11914a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "wind = -1\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "n_kqv = -1\n",
      "dropout = 0.2\n",
      "big_mlp = False\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 100\n",
      "Overriding: block_size = 128\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 256\n",
      "Overriding: n_kqv = 32\n",
      "Overriding: max_iters = 5000\n",
      "Overriding: lr_decay_iters = 5000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 2.64M\n",
      "num decayed parameter tensors: 18, with 2,670,848 parameters\n",
      "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.2636, val loss 4.2591\n",
      "iter 0: loss 4.2486, time 1701.79ms, mfu -100.00%\n",
      "iter 100: loss 2.5591, time 15.62ms, mfu 0.55%\n",
      "iter 200: loss 2.4592, time 15.63ms, mfu 0.55%\n",
      "step 250: train loss 2.3945, val loss 2.4084\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 300: loss 2.2829, time 15.63ms, mfu 0.55%\n",
      "iter 400: loss 2.1497, time 0.00ms, mfu 9.06%\n",
      "step 500: train loss 2.0231, val loss 2.0960\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 1.9995, time 1782.22ms, mfu 8.16%\n",
      "iter 600: loss 2.0062, time 0.00ms, mfu 15.92%\n",
      "iter 700: loss 1.8872, time 0.00ms, mfu 22.90%\n",
      "step 750: train loss 1.8314, val loss 1.9611\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 800: loss 1.8740, time 0.00ms, mfu 29.18%\n",
      "iter 900: loss 1.6763, time 0.00ms, mfu 34.84%\n",
      "step 1000: train loss 1.6953, val loss 1.8706\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.7464, time 1751.60ms, mfu 31.35%\n",
      "iter 1100: loss 1.6830, time 0.00ms, mfu 36.79%\n",
      "iter 1200: loss 1.6794, time 15.62ms, mfu 33.17%\n",
      "step 1250: train loss 1.6330, val loss 1.8161\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1300: loss 1.5961, time 0.00ms, mfu 38.42%\n",
      "iter 1400: loss 1.6217, time 0.00ms, mfu 43.16%\n",
      "step 1500: train loss 1.5733, val loss 1.7404\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.6124, time 1744.55ms, mfu 38.84%\n",
      "iter 1600: loss 1.5448, time 0.00ms, mfu 43.53%\n",
      "iter 1700: loss 1.5616, time 22.13ms, mfu 39.22%\n",
      "step 1750: train loss 1.5204, val loss 1.7077\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1800: loss 1.5244, time 15.63ms, mfu 35.35%\n",
      "iter 1900: loss 1.5826, time 22.13ms, mfu 31.85%\n",
      "step 2000: train loss 1.4882, val loss 1.6858\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.4546, time 1717.41ms, mfu 28.67%\n",
      "iter 2100: loss 1.5474, time 15.63ms, mfu 25.86%\n",
      "iter 2200: loss 1.5363, time 22.13ms, mfu 23.31%\n",
      "step 2250: train loss 1.4509, val loss 1.6551\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2300: loss 1.4771, time 0.00ms, mfu 29.55%\n",
      "iter 2400: loss 1.4887, time 0.00ms, mfu 35.17%\n",
      "step 2500: train loss 1.4284, val loss 1.6151\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.3576, time 1719.33ms, mfu 31.65%\n",
      "iter 2600: loss 1.4454, time 15.62ms, mfu 28.54%\n",
      "iter 2700: loss 1.4279, time 0.00ms, mfu 34.26%\n",
      "step 2750: train loss 1.4007, val loss 1.5935\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2800: loss 1.4364, time 0.00ms, mfu 39.41%\n",
      "iter 2900: loss 1.4303, time 0.00ms, mfu 44.04%\n",
      "step 3000: train loss 1.3593, val loss 1.5776\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.2185, time 1779.48ms, mfu 39.64%\n",
      "iter 3100: loss 1.3798, time 0.00ms, mfu 44.25%\n",
      "iter 3200: loss 1.3922, time 15.62ms, mfu 39.88%\n",
      "step 3250: train loss 1.3412, val loss 1.5653\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3300: loss 1.2755, time 15.62ms, mfu 35.94%\n",
      "iter 3400: loss 1.3466, time 15.63ms, mfu 32.40%\n",
      "step 3500: train loss 1.3237, val loss 1.5428\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.3076, time 1754.66ms, mfu 29.16%\n",
      "iter 3600: loss 1.3080, time 15.64ms, mfu 26.30%\n",
      "iter 3700: loss 1.2641, time 37.76ms, mfu 23.70%\n",
      "step 3750: train loss 1.2984, val loss 1.5338\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3800: loss 1.2660, time 6.51ms, mfu 21.46%\n",
      "iter 3900: loss 1.2796, time 15.63ms, mfu 19.36%\n",
      "step 4000: train loss 1.2773, val loss 1.5335\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.3782, time 1777.96ms, mfu 17.43%\n",
      "iter 4100: loss 1.3051, time 15.64ms, mfu 15.74%\n",
      "iter 4200: loss 1.3332, time 22.13ms, mfu 14.20%\n",
      "step 4250: train loss 1.2628, val loss 1.5215\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4300: loss 1.2358, time 0.00ms, mfu 21.36%\n",
      "iter 4400: loss 1.2377, time 15.63ms, mfu 19.28%\n",
      "step 4500: train loss 1.2495, val loss 1.5156\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.3900, time 1807.51ms, mfu 17.35%\n",
      "iter 4600: loss 1.3155, time 15.63ms, mfu 15.67%\n",
      "iter 4700: loss 1.2699, time 15.63ms, mfu 14.16%\n",
      "step 4750: train loss 1.2474, val loss 1.5148\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4800: loss 1.2103, time 0.00ms, mfu 21.31%\n",
      "iter 4900: loss 1.2266, time 0.00ms, mfu 27.76%\n",
      "step 5000: train loss 1.2280, val loss 1.5125\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 5000: loss 1.2073, time 1778.10ms, mfu 24.98%\n"
     ]
    }
   ],
   "source": [
    "%run train.py config/train_shakespeare_char.py --device=cuda --compile=False --eval_iters=200 --log_interval=100 --block_size=128 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=256 --n_kqv=32 --max_iters=5000 --lr_decay_iters=5000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5IliA0qt3Sv"
   },
   "source": [
    "Here, after 5000 iterations, we can see that the final validation loss is 1.5125. It's interesting that this actually achieves a slightly lower loss compared to the original model we ran, even though the alignment/attention block is smaller. This could mean that learning fewer, more general \"meanings\" of tokens works well. I'd assume that the two models work similarly, because rerunning them results in different losses, so the main conclusion to draw is that the difference in loss is not significant. We'd probably want to use this kind of model over the original one, since this runs faster due to the smaller attention block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4_UK0W2tHKI"
   },
   "source": [
    "Here is a small sample of what this model generates. This doesn't look much different from the original model's sample, which makes sense given that we didn't see too big of a difference in loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGWN__omsBOv",
    "outputId": "c123f9d7-9636-4086-8937-9f8e84842e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "Overriding: device = cuda\n",
      "number of parameters: 2.64M\n",
      "Loading meta from data\\shakespeare_char\\meta.pkl...\n",
      "\n",
      "Which if this writen in the time: sea hope to take\n",
      "On thy called heaven that us he had been the\n",
      "cause with my father to him. This is more in here,\n",
      "His nurse, and is enter'd will into drop the deed?\n",
      "I will not joy told in a wall--humble child:\n",
      "And the self, yet like us or done,\n",
      "And whom the selfsame of all out him prizes to the\n",
      "curious penitence of him the rest.\n",
      "\n",
      "KING RICHARD II:\n",
      "And he must not know on, for her shame of them;\n",
      "Leave too discapadin my brother's discontent.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Edward\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%run small_sample.py --out_dir=out-shakespeare-char --device=cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUmT2NYotp5q"
   },
   "source": [
    "Now let's run the model with n_kqv = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpoN-Kw_tvn9",
    "outputId": "1651525a-ce0c-464e-bd46-6fc941b1bdf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "wind = -1\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "n_kqv = -1\n",
      "dropout = 0.2\n",
      "big_mlp = False\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 100\n",
      "Overriding: block_size = 128\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 256\n",
      "Overriding: n_kqv = 8\n",
      "Overriding: max_iters = 5000\n",
      "Overriding: lr_decay_iters = 5000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 2.25M\n",
      "num decayed parameter tensors: 18, with 2,277,632 parameters\n",
      "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.1986, val loss 4.1972\n",
      "iter 0: loss 4.1864, time 1505.39ms, mfu -100.00%\n",
      "iter 100: loss 2.5221, time 15.63ms, mfu 0.47%\n",
      "iter 200: loss 2.4379, time 15.64ms, mfu 0.47%\n",
      "step 250: train loss 2.3983, val loss 2.4284\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 300: loss 2.3290, time 6.51ms, mfu 0.54%\n",
      "iter 400: loss 2.2558, time 0.00ms, mfu 7.89%\n",
      "step 500: train loss 2.1307, val loss 2.1712\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.1128, time 1734.12ms, mfu 7.11%\n",
      "iter 600: loss 2.0312, time 15.64ms, mfu 6.44%\n",
      "iter 700: loss 2.0252, time 15.63ms, mfu 5.85%\n",
      "step 750: train loss 1.9114, val loss 2.0092\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 800: loss 1.9491, time 0.00ms, mfu 12.67%\n",
      "iter 900: loss 1.7898, time 0.00ms, mfu 18.82%\n",
      "step 1000: train loss 1.7735, val loss 1.9200\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.7687, time 1667.76ms, mfu 16.94%\n",
      "iter 1100: loss 1.7988, time 15.63ms, mfu 15.29%\n",
      "iter 1200: loss 1.7082, time 15.63ms, mfu 13.81%\n",
      "step 1250: train loss 1.6980, val loss 1.8567\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1300: loss 1.6570, time 15.63ms, mfu 12.47%\n",
      "iter 1400: loss 1.6185, time 0.00ms, mfu 18.64%\n",
      "step 1500: train loss 1.6244, val loss 1.7972\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.6043, time 1751.10ms, mfu 16.78%\n",
      "iter 1600: loss 1.6247, time 0.00ms, mfu 22.51%\n",
      "iter 1700: loss 1.6903, time 0.00ms, mfu 27.67%\n",
      "step 1750: train loss 1.5756, val loss 1.7570\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1800: loss 1.4677, time 6.50ms, mfu 25.02%\n",
      "iter 1900: loss 1.5375, time 0.00ms, mfu 29.93%\n",
      "step 2000: train loss 1.5314, val loss 1.7171\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.5733, time 1676.18ms, mfu 26.93%\n",
      "iter 2100: loss 1.4808, time 0.00ms, mfu 31.65%\n",
      "iter 2200: loss 1.5103, time 15.63ms, mfu 28.54%\n",
      "step 2250: train loss 1.4983, val loss 1.6812\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2300: loss 1.4410, time 17.14ms, mfu 25.72%\n",
      "iter 2400: loss 1.4054, time 0.00ms, mfu 30.56%\n",
      "step 2500: train loss 1.4628, val loss 1.6406\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.4958, time 1610.10ms, mfu 27.51%\n",
      "iter 2600: loss 1.3794, time 13.20ms, mfu 24.81%\n",
      "iter 2700: loss 1.3885, time 13.41ms, mfu 22.39%\n",
      "step 2750: train loss 1.4255, val loss 1.6206\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2800: loss 1.5211, time 0.00ms, mfu 27.56%\n",
      "iter 2900: loss 1.4278, time 16.22ms, mfu 24.85%\n",
      "step 3000: train loss 1.4038, val loss 1.5979\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.4579, time 1763.51ms, mfu 22.37%\n",
      "iter 3100: loss 1.4906, time 13.32ms, mfu 20.18%\n",
      "iter 3200: loss 1.4126, time 8.00ms, mfu 18.26%\n",
      "step 3250: train loss 1.3823, val loss 1.5762\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3300: loss 1.3743, time 15.62ms, mfu 16.48%\n",
      "iter 3400: loss 1.3351, time 15.64ms, mfu 14.88%\n",
      "step 3500: train loss 1.3566, val loss 1.5579\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.2916, time 1664.91ms, mfu 13.39%\n",
      "iter 3600: loss 1.4381, time 15.63ms, mfu 12.10%\n",
      "iter 3700: loss 1.4447, time 15.63ms, mfu 10.94%\n",
      "step 3750: train loss 1.3289, val loss 1.5620\n",
      "iter 3800: loss 1.3626, time 0.00ms, mfu 17.25%\n",
      "iter 3900: loss 1.3114, time 15.63ms, mfu 15.58%\n",
      "step 4000: train loss 1.3208, val loss 1.5385\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.3521, time 1654.74ms, mfu 14.02%\n",
      "iter 4100: loss 1.3352, time 15.62ms, mfu 12.66%\n",
      "iter 4200: loss 1.2980, time 15.63ms, mfu 11.44%\n",
      "step 4250: train loss 1.3092, val loss 1.5326\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4300: loss 1.3153, time 0.00ms, mfu 17.71%\n",
      "iter 4400: loss 1.3558, time 15.63ms, mfu 15.99%\n",
      "step 4500: train loss 1.2908, val loss 1.5207\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.3179, time 1687.08ms, mfu 14.39%\n",
      "iter 4600: loss 1.3492, time 0.00ms, mfu 20.36%\n",
      "iter 4700: loss 1.2645, time 0.00ms, mfu 25.74%\n",
      "step 4750: train loss 1.2830, val loss 1.5152\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4800: loss 1.1974, time 15.63ms, mfu 23.21%\n",
      "iter 4900: loss 1.2684, time 0.00ms, mfu 28.30%\n",
      "step 5000: train loss 1.2774, val loss 1.5167\n",
      "iter 5000: loss 1.2960, time 1679.16ms, mfu 25.47%\n"
     ]
    }
   ],
   "source": [
    "%run train.py config/train_shakespeare_char.py --device=cuda --compile=False --eval_iters=200 --log_interval=100 --block_size=128 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=256 --n_kqv=8 --max_iters=5000 --lr_decay_iters=5000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxU2_RpRt87J"
   },
   "source": [
    "After 5000 iterations, we see that the lowest validation loss is 1.5152. This is initially surprising, because I would've expected this to achieve a significantly worse loss. However, it even slightly outperforms the model with the keys, queries, and values set to 32 dimensions. This difference in loss could also just be due to chance, since the loss varies between different runs. However, this seems to indicate minimal difference in loss between having the keys, queries, and values be 32-dimensional versus 8-dimensional. We'd probably want to choose this model, since it achieves a similar loss while running faster. One thing to note is that the 32D model achieves a lower training loss than the 8D model (1.3821 for 32D and 1.3976 for 8D), so it seems that higher model complexity leads to overfitting. Intuitively, this sounds reasonable because we're only generating characters, and there's only so much meaning that a character can encode as opposed to sub-words. With dimensionality of 64, our original model likely still isn't able to understand the plot of the story, and still is only generating words that are real, but don't actually fit together. I suspect that if we ran a much larger model (especially if it used sub-words instead of characters as tokens), it would suffer much more from reducing the dimensionality of the keys, queries, and values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUzv7jZNt_o8"
   },
   "source": [
    "Here is a small sample of what this model generates. It also looks similar to the two previous samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKgfv49Ut_AM",
    "outputId": "eddc3b2d-c1e2-48ae-ea18-85344ba436a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "Overriding: device = cuda\n",
      "number of parameters: 2.25M\n",
      "Loading meta from data\\shakespeare_char\\meta.pkl...\n",
      "\n",
      "\n",
      "KING RICHARD II:\n",
      "Shall I, but be set body to take and my called\n",
      "My arm that unquit to be to late,\n",
      "But when you fall to his man own proof,\n",
      "And both it now in a grieve--\n",
      "But will it there by the duke presence with\n",
      "self-jodice of the milde in the princess,\n",
      "why have not know passion of themself.\n",
      "\n",
      "BRUTUS:\n",
      "No, so what evil her at thou will not in his son\n",
      "To hear unto and thrusted so;\n",
      "And he must not male of a fairer services.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Sadam, the Earl of England and and Hereford,\n",
      "And for th\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%run small_sample.py --out_dir=out-shakespeare-char --device=cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we've learned from this is that the embedding for a single character might not carry much meaning, and we might not even need a very high dimensional space to capture the different meanings of the characters. A large space the size of the alphabet might essentially learn something similar to one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGt4pXl2uFp8"
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I implemented a mask very similar to the future masking, except on the past. I got rid of the flash attention because I wouldn't be able to modify their implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowCausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "        self.register_buffer(\"window\", torch.tril(torch.ones(config.block_size + config.wind, config.block_size + config.wind))[:config.block_size, config.wind:]\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill((self.bias - self.window)[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUaH5VNlQ-GV",
    "outputId": "f47ba450-af91-4956-bec5-b53f90372f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "wind = -1\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "n_kqv = -1\n",
      "dropout = 0.2\n",
      "big_mlp = False\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 100\n",
      "Overriding: block_size = 128\n",
      "Overriding: wind = 100\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 5000\n",
      "Overriding: lr_decay_iters = 5000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,136 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.2065, val loss 4.2036\n",
      "iter 0: loss 4.2192, time 1727.72ms, mfu -100.00%\n",
      "iter 100: loss 2.5787, time 7.98ms, mfu 0.34%\n",
      "iter 200: loss 2.4613, time 10.00ms, mfu 0.33%\n",
      "step 250: train loss 2.4438, val loss 2.4547\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 300: loss 2.4365, time 15.34ms, mfu 0.32%\n",
      "iter 400: loss 2.3163, time 3.26ms, mfu 0.37%\n",
      "step 500: train loss 2.2868, val loss 2.3059\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.1735, time 1642.03ms, mfu 0.33%\n",
      "iter 600: loss 2.1630, time 15.63ms, mfu 0.31%\n",
      "iter 700: loss 2.1547, time 8.76ms, mfu 0.31%\n",
      "step 750: train loss 2.0895, val loss 2.1441\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 800: loss 2.0657, time 0.00ms, mfu 3.02%\n",
      "iter 900: loss 2.0110, time 15.63ms, mfu 2.74%\n",
      "step 1000: train loss 1.9638, val loss 2.0523\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.9478, time 1747.91ms, mfu 2.46%\n",
      "iter 1100: loss 1.9856, time 0.00ms, mfu 4.95%\n",
      "iter 1200: loss 1.8492, time 6.50ms, mfu 4.50%\n",
      "step 1250: train loss 1.8441, val loss 1.9688\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1300: loss 1.6946, time 8.69ms, mfu 4.08%\n",
      "iter 1400: loss 1.8637, time 9.20ms, mfu 3.70%\n",
      "step 1500: train loss 1.7673, val loss 1.8958\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.8389, time 1670.14ms, mfu 3.33%\n",
      "iter 1600: loss 1.8178, time 0.00ms, mfu 5.74%\n",
      "iter 1700: loss 1.6348, time 0.00ms, mfu 7.90%\n",
      "step 1750: train loss 1.7060, val loss 1.8524\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1800: loss 1.7960, time 15.85ms, mfu 7.13%\n",
      "iter 1900: loss 1.6722, time 0.00ms, mfu 9.15%\n",
      "step 2000: train loss 1.6566, val loss 1.8202\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.6303, time 1705.19ms, mfu 8.24%\n",
      "iter 2100: loss 1.5421, time 8.45ms, mfu 7.45%\n",
      "iter 2200: loss 1.5986, time 13.11ms, mfu 6.72%\n",
      "step 2250: train loss 1.6052, val loss 1.7881\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2300: loss 1.4790, time 15.85ms, mfu 6.07%\n",
      "iter 2400: loss 1.5062, time 8.35ms, mfu 5.49%\n",
      "step 2500: train loss 1.5707, val loss 1.7488\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.5316, time 1770.92ms, mfu 4.94%\n",
      "iter 2600: loss 1.4952, time 2.05ms, mfu 4.58%\n",
      "iter 2700: loss 1.5190, time 0.00ms, mfu 6.86%\n",
      "step 2750: train loss 1.5365, val loss 1.6979\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2800: loss 1.5352, time 0.00ms, mfu 8.91%\n",
      "iter 2900: loss 1.4366, time 15.64ms, mfu 8.04%\n",
      "step 3000: train loss 1.5120, val loss 1.6791\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.5377, time 1465.06ms, mfu 7.23%\n",
      "iter 3100: loss 1.5462, time 15.63ms, mfu 6.53%\n",
      "iter 3200: loss 1.5178, time 15.62ms, mfu 5.89%\n",
      "step 3250: train loss 1.4879, val loss 1.6672\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3300: loss 1.4908, time 16.03ms, mfu 5.32%\n",
      "iter 3400: loss 1.5405, time 15.66ms, mfu 4.80%\n",
      "step 3500: train loss 1.4660, val loss 1.6556\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.4090, time 1447.89ms, mfu 4.32%\n",
      "iter 3600: loss 1.5053, time 15.63ms, mfu 3.91%\n",
      "iter 3700: loss 1.4316, time 15.63ms, mfu 3.54%\n",
      "step 3750: train loss 1.4417, val loss 1.6279\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3800: loss 1.5711, time 6.50ms, mfu 3.22%\n",
      "iter 3900: loss 1.3465, time 15.62ms, mfu 2.92%\n",
      "step 4000: train loss 1.4357, val loss 1.6261\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.3715, time 1558.46ms, mfu 2.63%\n",
      "iter 4100: loss 1.4902, time 0.00ms, mfu 5.10%\n",
      "iter 4200: loss 1.4731, time 0.00ms, mfu 7.33%\n",
      "step 4250: train loss 1.4207, val loss 1.6087\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4300: loss 1.4357, time 15.63ms, mfu 6.61%\n",
      "iter 4400: loss 1.4354, time 0.00ms, mfu 8.69%\n",
      "step 4500: train loss 1.4015, val loss 1.6048\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.3578, time 1548.47ms, mfu 7.82%\n",
      "iter 4600: loss 1.4464, time 15.84ms, mfu 7.06%\n",
      "iter 4700: loss 1.4150, time 13.21ms, mfu 6.37%\n",
      "step 4750: train loss 1.3948, val loss 1.6060\n",
      "iter 4800: loss 1.3935, time 8.00ms, mfu 5.77%\n",
      "iter 4900: loss 1.3631, time 0.00ms, mfu 7.93%\n",
      "step 5000: train loss 1.3951, val loss 1.5868\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 5000: loss 1.3544, time 1490.34ms, mfu 7.14%\n"
     ]
    }
   ],
   "source": [
    "%run train.py config/train_shakespeare_char.py --device=cuda --compile=False --eval_iters=200 --log_interval=100 --block_size=128 --wind=100 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=5000 --lr_decay_iters=5000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our lowest validation loss is 1.5868, somewhat worse than the original model. This makes sense because we're not dense attention anymore, and some later tokens can't attend to the earliest ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample of what this model produces. It looks very similar to those before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mY3YraT3QMGP",
    "outputId": "33976df0-094f-46fe-b4f1-3f5a40a96462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "Overriding: device = cuda\n",
      "number of parameters: 0.80M\n",
      "Loading meta from data\\shakespeare_char\\meta.pkl...\n",
      "\n",
      "\n",
      "All that king's and is and the dies, but our hand,\n",
      "Say the galden bark that us arm to bereding\n",
      "Hate away will not daughterous\n",
      "Your proceedstering maided;\n",
      "Which misterible, will it the overture.\n",
      "\n",
      "WARWICK:\n",
      "Alas, my else noble lie-husbacce in the princess,\n",
      "why hold not not doth in destring the light.\n",
      "\n",
      "KING RICHARD III:\n",
      "Then are do river in him, strong of his burther\n",
      "In crupt for arms. Long for thy fled, the shall be bod?\n",
      "\n",
      "KING RICHARD III:\n",
      "Hark, and the Edward's in courtear tears,--\n",
      "\n",
      "SICINIUS:\n",
      "Now\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%run small_sample.py --out_dir=out-shakespeare-char --device=cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9dFSCIhQE38",
    "outputId": "41530f74-d2e7-4ebd-b738-ea1b4ed4fb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "wind = -1\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "n_kqv = -1\n",
      "dropout = 0.2\n",
      "big_mlp = False\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 100\n",
      "Overriding: block_size = 128\n",
      "Overriding: wind = 10\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 5000\n",
      "Overriding: lr_decay_iters = 5000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,136 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.2074, val loss 4.2050\n",
      "iter 0: loss 4.2187, time 1607.00ms, mfu -100.00%\n",
      "iter 100: loss 2.5037, time 15.42ms, mfu 0.18%\n",
      "iter 200: loss 2.3479, time 0.00ms, mfu 2.90%\n",
      "step 250: train loss 2.3297, val loss 2.3320\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 300: loss 2.2839, time 0.00ms, mfu 5.35%\n",
      "iter 400: loss 2.1186, time 0.00ms, mfu 7.55%\n",
      "step 500: train loss 2.0229, val loss 2.0915\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 1.9065, time 1585.84ms, mfu 6.79%\n",
      "iter 600: loss 1.9240, time 15.62ms, mfu 6.13%\n",
      "iter 700: loss 1.8590, time 0.00ms, mfu 8.26%\n",
      "step 750: train loss 1.8578, val loss 1.9591\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 800: loss 1.8182, time 13.61ms, mfu 7.45%\n",
      "iter 900: loss 1.7591, time 13.48ms, mfu 6.73%\n",
      "step 1000: train loss 1.7359, val loss 1.8859\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.7179, time 1553.61ms, mfu 6.05%\n",
      "iter 1100: loss 1.7908, time 0.00ms, mfu 8.19%\n",
      "iter 1200: loss 1.6722, time 15.63ms, mfu 7.39%\n",
      "step 1250: train loss 1.6661, val loss 1.8330\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1300: loss 1.5125, time 15.64ms, mfu 6.66%\n",
      "iter 1400: loss 1.7335, time 15.85ms, mfu 6.02%\n",
      "step 1500: train loss 1.6182, val loss 1.7843\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.6876, time 1549.07ms, mfu 5.41%\n",
      "iter 1600: loss 1.6836, time 8.01ms, mfu 4.91%\n",
      "iter 1700: loss 1.4996, time 15.63ms, mfu 4.43%\n",
      "step 1750: train loss 1.5719, val loss 1.7360\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1800: loss 1.6457, time 16.03ms, mfu 4.01%\n",
      "iter 1900: loss 1.5431, time 15.61ms, mfu 3.62%\n",
      "step 2000: train loss 1.5422, val loss 1.7233\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.5361, time 1604.30ms, mfu 3.26%\n",
      "iter 2100: loss 1.4136, time 15.63ms, mfu 2.95%\n",
      "iter 2200: loss 1.4926, time 15.62ms, mfu 2.67%\n",
      "step 2250: train loss 1.4986, val loss 1.6985\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2300: loss 1.3676, time 15.62ms, mfu 2.42%\n",
      "iter 2400: loss 1.4199, time 0.00ms, mfu 4.92%\n",
      "step 2500: train loss 1.4799, val loss 1.6807\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.4515, time 1573.01ms, mfu 4.43%\n",
      "iter 2600: loss 1.3780, time 15.63ms, mfu 4.00%\n",
      "iter 2700: loss 1.4169, time 0.00ms, mfu 6.34%\n",
      "step 2750: train loss 1.4532, val loss 1.6417\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2800: loss 1.4517, time 15.82ms, mfu 5.72%\n",
      "iter 2900: loss 1.3427, time 15.64ms, mfu 5.17%\n",
      "step 3000: train loss 1.4297, val loss 1.6254\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.4536, time 1611.42ms, mfu 4.65%\n",
      "iter 3100: loss 1.4530, time 15.63ms, mfu 4.20%\n",
      "iter 3200: loss 1.4378, time 0.00ms, mfu 6.52%\n",
      "step 3250: train loss 1.4090, val loss 1.6146\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3300: loss 1.4261, time 15.85ms, mfu 5.89%\n",
      "iter 3400: loss 1.4480, time 2.31ms, mfu 5.41%\n",
      "step 3500: train loss 1.3923, val loss 1.6152\n",
      "iter 3500: loss 1.3723, time 1459.21ms, mfu 4.87%\n",
      "iter 3600: loss 1.4443, time 6.50ms, mfu 4.43%\n",
      "iter 3700: loss 1.3748, time 9.00ms, mfu 4.01%\n",
      "step 3750: train loss 1.3714, val loss 1.5921\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3800: loss 1.5210, time 9.75ms, mfu 3.64%\n",
      "iter 3900: loss 1.2878, time 15.62ms, mfu 3.29%\n",
      "step 4000: train loss 1.3621, val loss 1.5873\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.2723, time 1464.44ms, mfu 2.96%\n",
      "iter 4100: loss 1.4290, time 15.88ms, mfu 2.68%\n",
      "iter 4200: loss 1.4032, time 15.64ms, mfu 2.43%\n",
      "step 4250: train loss 1.3503, val loss 1.5717\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4300: loss 1.3765, time 15.63ms, mfu 2.21%\n",
      "iter 4400: loss 1.3635, time 6.50ms, mfu 2.03%\n",
      "step 4500: train loss 1.3341, val loss 1.5725\n",
      "iter 4500: loss 1.2896, time 1452.21ms, mfu 1.83%\n",
      "iter 4600: loss 1.3663, time 0.00ms, mfu 4.38%\n",
      "iter 4700: loss 1.3385, time 15.63ms, mfu 3.96%\n",
      "step 4750: train loss 1.3280, val loss 1.5780\n",
      "iter 4800: loss 1.3493, time 0.00ms, mfu 6.30%\n",
      "iter 4900: loss 1.3044, time 0.00ms, mfu 8.41%\n",
      "step 5000: train loss 1.3284, val loss 1.5632\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 5000: loss 1.3008, time 1571.02ms, mfu 7.57%\n"
     ]
    }
   ],
   "source": [
    "%run train.py config/train_shakespeare_char.py --device=cuda --compile=False --eval_iters=200 --log_interval=100 --block_size=128 --wind=10 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=5000 --lr_decay_iters=5000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the final validation loss is 1.5632, which is lower than when we use a size 100 sliding window. This is somewhat surprising to me, because I would've expected shorter windows to cause higher loss. However, one explanation could be that since we're only encoding tokens as single characters, there's actually very little meaning to be found in a relationship between, say, the first character and the 100th character. The model may try to find a relationship where there isn't really one, and thus worsens the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample of what this model produces. It looks very similar to those before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tn2qylO4QPXn",
    "outputId": "d313cfb6-1d3a-4d16-ebd8-88c9db0c1269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "Overriding: device = cuda\n",
      "number of parameters: 0.80M\n",
      "Loading meta from data\\shakespeare_char\\meta.pkl...\n",
      "\n",
      "\n",
      "Clown:\n",
      "Redide with the sweeter and thanks to take and my call'd\n",
      "My worth heart him to back the waters.\n",
      "\n",
      "MENENIUS:\n",
      "I come to hear them!\n",
      "\n",
      "Citizens:\n",
      "His noble caught well. I hear them, like the deeping of his eyes,\n",
      "Well and tears the mark with a cherish'd,\n",
      "why holy name noble to gold him.\n",
      "\n",
      "LEONTES:\n",
      "I look your what evils, the most right in him,\n",
      "And of my heart\n",
      "To king three for on; and there three fled\n",
      "At that Prince with the good of the discords and his great hope courts.\n",
      "\n",
      "ANGELO:\n",
      "Why, for I vali\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%run small_sample.py --out_dir=out-shakespeare-char --device=cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfuMDm_pQE_5",
    "outputId": "d0cd9304-3604-4ec9-e3f9-04c1a1a6ecf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "wind = -1\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "n_kqv = -1\n",
      "dropout = 0.2\n",
      "big_mlp = False\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 100\n",
      "Overriding: block_size = 128\n",
      "Overriding: wind = 3\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 5000\n",
      "Overriding: lr_decay_iters = 5000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 811,136 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.2068, val loss 4.2054\n",
      "iter 0: loss 4.2210, time 1640.10ms, mfu -100.00%\n",
      "iter 100: loss 2.3236, time 6.50ms, mfu 0.41%\n",
      "iter 200: loss 2.0858, time 15.85ms, mfu 0.39%\n",
      "step 250: train loss 2.0239, val loss 2.0604\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 300: loss 1.9592, time 15.38ms, mfu 0.37%\n",
      "iter 400: loss 1.9286, time 0.00ms, mfu 3.07%\n",
      "step 500: train loss 1.8321, val loss 1.9575\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 1.7010, time 1580.96ms, mfu 2.76%\n",
      "iter 600: loss 1.7575, time 0.00ms, mfu 5.23%\n",
      "iter 700: loss 1.7008, time 15.66ms, mfu 4.72%\n",
      "step 750: train loss 1.7211, val loss 1.8456\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 800: loss 1.7114, time 15.63ms, mfu 4.27%\n",
      "iter 900: loss 1.6743, time 0.00ms, mfu 6.58%\n",
      "step 1000: train loss 1.6539, val loss 1.8259\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.6255, time 1579.57ms, mfu 5.92%\n",
      "iter 1100: loss 1.7258, time 13.22ms, mfu 5.35%\n",
      "iter 1200: loss 1.5996, time 14.13ms, mfu 4.83%\n",
      "step 1250: train loss 1.5999, val loss 1.7632\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1300: loss 1.4674, time 0.00ms, mfu 7.09%\n",
      "iter 1400: loss 1.6646, time 18.55ms, mfu 6.39%\n",
      "step 1500: train loss 1.5703, val loss 1.7380\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.6388, time 1618.90ms, mfu 5.75%\n",
      "iter 1600: loss 1.6512, time 15.84ms, mfu 5.20%\n",
      "iter 1700: loss 1.4581, time 15.42ms, mfu 4.69%\n",
      "step 1750: train loss 1.5309, val loss 1.6979\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1800: loss 1.6094, time 13.45ms, mfu 4.24%\n",
      "iter 1900: loss 1.5087, time 0.00ms, mfu 6.56%\n",
      "step 2000: train loss 1.5181, val loss 1.6913\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.4938, time 1711.30ms, mfu 5.90%\n",
      "iter 2100: loss 1.4008, time 8.09ms, mfu 5.35%\n",
      "iter 2200: loss 1.4746, time 15.87ms, mfu 4.83%\n",
      "step 2250: train loss 1.4750, val loss 1.6761\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2300: loss 1.3402, time 0.00ms, mfu 7.08%\n",
      "iter 2400: loss 1.4240, time 0.00ms, mfu 9.11%\n",
      "step 2500: train loss 1.4598, val loss 1.6484\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.4359, time 1650.44ms, mfu 8.20%\n",
      "iter 2600: loss 1.3837, time 0.00ms, mfu 10.12%\n",
      "iter 2700: loss 1.4108, time 0.00ms, mfu 11.85%\n",
      "step 2750: train loss 1.4390, val loss 1.6294\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2800: loss 1.4361, time 0.00ms, mfu 13.40%\n",
      "iter 2900: loss 1.3411, time 15.62ms, mfu 12.08%\n",
      "step 3000: train loss 1.4190, val loss 1.6031\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.4637, time 1586.68ms, mfu 10.87%\n",
      "iter 3100: loss 1.4623, time 15.62ms, mfu 9.80%\n",
      "iter 3200: loss 1.4259, time 0.00ms, mfu 11.56%\n",
      "step 3250: train loss 1.4045, val loss 1.6052\n",
      "iter 3300: loss 1.3955, time 0.00ms, mfu 13.14%\n",
      "iter 3400: loss 1.4474, time 15.33ms, mfu 11.84%\n",
      "step 3500: train loss 1.3883, val loss 1.5948\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3500: loss 1.3652, time 1531.11ms, mfu 10.66%\n",
      "iter 3600: loss 1.4416, time 15.63ms, mfu 9.61%\n",
      "iter 3700: loss 1.3672, time 0.00ms, mfu 11.39%\n",
      "step 3750: train loss 1.3690, val loss 1.5751\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3800: loss 1.5119, time 0.00ms, mfu 12.99%\n",
      "iter 3900: loss 1.3074, time 0.00ms, mfu 14.43%\n",
      "step 4000: train loss 1.3672, val loss 1.5741\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.2756, time 1559.72ms, mfu 12.98%\n",
      "iter 4100: loss 1.4153, time 15.64ms, mfu 11.70%\n",
      "iter 4200: loss 1.3955, time 0.00ms, mfu 13.27%\n",
      "step 4250: train loss 1.3564, val loss 1.5620\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4300: loss 1.3567, time 6.51ms, mfu 11.99%\n",
      "iter 4400: loss 1.3738, time 15.63ms, mfu 10.80%\n",
      "step 4500: train loss 1.3418, val loss 1.5587\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.3131, time 1507.68ms, mfu 9.72%\n",
      "iter 4600: loss 1.3776, time 0.00ms, mfu 11.49%\n",
      "iter 4700: loss 1.3463, time 15.62ms, mfu 10.36%\n",
      "step 4750: train loss 1.3369, val loss 1.5672\n",
      "iter 4800: loss 1.3360, time 5.50ms, mfu 9.37%\n",
      "iter 4900: loss 1.3335, time 15.64ms, mfu 8.45%\n",
      "step 5000: train loss 1.3358, val loss 1.5440\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 5000: loss 1.3148, time 1572.95ms, mfu 7.61%\n"
     ]
    }
   ],
   "source": [
    "%run train.py config/train_shakespeare_char.py --device=cuda --compile=False --eval_iters=200 --log_interval=100 --block_size=128 --wind=3 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=5000 --lr_decay_iters=5000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final validation loss here is 1.5440, even lower than when my window was of size 10. The same explanation as before may apply in this case, because there might only be a significant relationship between maybe the last two or three letters when predicting the next letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample of what this model produces. It looks very similar to those before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5d5Vs6AQQSp",
    "outputId": "6ce381fa-7c87-4de5-b9fa-18299267271f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "Overriding: device = cuda\n",
      "number of parameters: 0.80M\n",
      "Loading meta from data\\shakespeare_char\\meta.pkl...\n",
      "\n",
      "\n",
      "All:\n",
      "If this world, is not the disposition of any ready me?\n",
      "\n",
      "Third York, I put of her bar die.\n",
      "\n",
      "GLOUCESTER:\n",
      "What's not how make\n",
      "the moof in heaven! What like their eyes accessarise in overtach.\n",
      "\n",
      "WARWICK:\n",
      "Alas, my mislike than this mild him speak; and the tyrant to see thou tell thee may should that\n",
      "more counsel, evils, the modestion with a choler, my lord.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "I'll sworn will the pinless than first?\n",
      "\n",
      "KING HENRY VI:\n",
      "Hark,\n",
      "And all day Warwick, stay in courtear thy very soldiers care,\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%run small_sample.py --out_dir=out-shakespeare-char --device=cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZP6q4CakN0A"
   },
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc1    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_fc2    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.c_fc1(x)\n",
    "        x2 = self.c_fc2(x)\n",
    "        x = x1*x2\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3NVUSL4ncve"
   },
   "source": [
    "After modifying the MLP in the model, it will have some more parameters just due to the extra layer added in the MLP. This could result in a slight improvement in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNRXfMVykPCm",
    "outputId": "6d2f466a-f347-4425-b5d7-e4292d9ad535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "wind = -1\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "n_kqv = -1\n",
      "dropout = 0.2\n",
      "big_mlp = False\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 100\n",
      "Overriding: block_size = 128\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: big_mlp = True\n",
      "Overriding: max_iters = 5000\n",
      "Overriding: lr_decay_iters = 5000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 1,536\n",
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 1.06M\n",
      "num decayed parameter tensors: 22, with 1,073,280 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.1888, val loss 4.1844\n",
      "iter 0: loss 4.1912, time 1730.50ms, mfu -100.00%\n",
      "iter 100: loss 2.6225, time 9.04ms, mfu 0.38%\n",
      "iter 200: loss 2.4730, time 8.82ms, mfu 0.39%\n",
      "step 250: train loss 2.4224, val loss 2.4262\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 300: loss 2.4101, time 13.48ms, mfu 0.37%\n",
      "iter 400: loss 2.3223, time 21.97ms, mfu 0.35%\n",
      "step 500: train loss 2.1969, val loss 2.2444\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.1828, time 1619.37ms, mfu 0.32%\n",
      "iter 600: loss 2.0348, time 9.01ms, mfu 0.32%\n",
      "iter 700: loss 1.9580, time 2.25ms, mfu 0.44%\n",
      "step 750: train loss 1.9444, val loss 2.0498\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 800: loss 1.9979, time 1.01ms, mfu 0.71%\n",
      "iter 900: loss 1.8293, time 0.00ms, mfu 4.15%\n",
      "step 1000: train loss 1.7783, val loss 1.9149\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1000: loss 1.8062, time 1506.56ms, mfu 3.74%\n",
      "iter 1100: loss 1.8524, time 13.65ms, mfu 3.39%\n",
      "iter 1200: loss 1.5949, time 8.08ms, mfu 3.09%\n",
      "step 1250: train loss 1.6871, val loss 1.8350\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1300: loss 1.5976, time 13.32ms, mfu 2.81%\n",
      "iter 1400: loss 1.7720, time 11.38ms, mfu 2.56%\n",
      "step 1500: train loss 1.5988, val loss 1.7789\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1500: loss 1.4752, time 1594.43ms, mfu 2.30%\n",
      "iter 1600: loss 1.5146, time 7.75ms, mfu 2.12%\n",
      "iter 1700: loss 1.5652, time 0.00ms, mfu 5.42%\n",
      "step 1750: train loss 1.5604, val loss 1.7436\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 1800: loss 1.4845, time 0.00ms, mfu 8.39%\n",
      "iter 1900: loss 1.5197, time 15.63ms, mfu 7.57%\n",
      "step 2000: train loss 1.5210, val loss 1.7102\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2000: loss 1.4894, time 1579.68ms, mfu 6.82%\n",
      "iter 2100: loss 1.4594, time 15.63ms, mfu 6.16%\n",
      "iter 2200: loss 1.4713, time 15.64ms, mfu 5.56%\n",
      "step 2250: train loss 1.4809, val loss 1.6580\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2300: loss 1.3952, time 6.51ms, mfu 5.06%\n",
      "iter 2400: loss 1.4226, time 15.62ms, mfu 4.58%\n",
      "step 2500: train loss 1.4445, val loss 1.6570\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2500: loss 1.4761, time 1615.35ms, mfu 4.12%\n",
      "iter 2600: loss 1.4496, time 15.63ms, mfu 3.73%\n",
      "iter 2700: loss 1.3663, time 15.63ms, mfu 3.38%\n",
      "step 2750: train loss 1.4200, val loss 1.6132\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 2800: loss 1.3648, time 15.64ms, mfu 3.06%\n",
      "iter 2900: loss 1.4064, time 15.63ms, mfu 2.78%\n",
      "step 3000: train loss 1.4049, val loss 1.6069\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3000: loss 1.3638, time 1578.23ms, mfu 2.50%\n",
      "iter 3100: loss 1.3287, time 0.00ms, mfu 5.76%\n",
      "iter 3200: loss 1.4218, time 22.14ms, mfu 5.20%\n",
      "step 3250: train loss 1.3786, val loss 1.5749\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3300: loss 1.4026, time 0.00ms, mfu 8.20%\n",
      "iter 3400: loss 1.3770, time 0.00ms, mfu 10.89%\n",
      "step 3500: train loss 1.3546, val loss 1.5847\n",
      "iter 3500: loss 1.3706, time 1477.16ms, mfu 9.80%\n",
      "iter 3600: loss 1.4913, time 0.00ms, mfu 12.33%\n",
      "iter 3700: loss 1.3152, time 6.50ms, mfu 11.15%\n",
      "step 3750: train loss 1.3380, val loss 1.5685\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 3800: loss 1.3367, time 0.00ms, mfu 13.55%\n",
      "iter 3900: loss 1.2849, time 15.62ms, mfu 12.22%\n",
      "step 4000: train loss 1.3246, val loss 1.5471\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4000: loss 1.2975, time 1498.24ms, mfu 11.00%\n",
      "iter 4100: loss 1.3309, time 1.37ms, mfu 10.13%\n",
      "iter 4200: loss 1.2357, time 9.00ms, mfu 9.16%\n",
      "step 4250: train loss 1.3159, val loss 1.5343\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4300: loss 1.3399, time 11.34ms, mfu 8.27%\n",
      "iter 4400: loss 1.3222, time 11.59ms, mfu 7.48%\n",
      "step 4500: train loss 1.3029, val loss 1.5311\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4500: loss 1.2594, time 1596.38ms, mfu 6.73%\n",
      "iter 4600: loss 1.3455, time 0.00ms, mfu 9.57%\n",
      "iter 4700: loss 1.3151, time 8.94ms, mfu 8.65%\n",
      "step 4750: train loss 1.2967, val loss 1.5232\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 4800: loss 1.3229, time 0.00ms, mfu 11.30%\n",
      "iter 4900: loss 1.2910, time 0.00ms, mfu 13.68%\n",
      "step 5000: train loss 1.2905, val loss 1.5256\n",
      "iter 5000: loss 1.2240, time 1476.13ms, mfu 12.31%\n"
     ]
    }
   ],
   "source": [
    "%run train.py config/train_shakespeare_char.py --device=cuda --compile=False --eval_iters=200 --log_interval=100 --block_size=128 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --big_mlp=True --max_iters=5000 --lr_decay_iters=5000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6b8uPp4mTw6"
   },
   "source": [
    "After 5000 iterations, we get a lowest validation loss of 1.5232. This doesn't improve upon the original model's loss, which could indicate that we might be overfitting. I see that the validation loss at step 4750 is lower than the validation at step 5000, and the training loss is much lower than the validation loss, which is a sign of overfitting. To truly test this model, we might want to use a larger validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MentB6_MnsaN"
   },
   "source": [
    "Here's a sample of what this model produces. It looks very similar to those before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9gFQHgemXmN",
    "outputId": "d41c82ff-cc45-4bcb-9b5d-16f1a0796f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "Overriding: device = cuda\n",
      "number of parameters: 1.06M\n",
      "Loading meta from data\\shakespeare_char\\meta.pkl...\n",
      "\n",
      "And they bear own for some blood: and but our care\n",
      "On the day of heaven that usurply but heaven\n",
      "Hate away way.\n",
      "\n",
      "HERMIONE:\n",
      "In heavens, to this common wherein, and if enter mine\n",
      "Still in overtain.\n",
      "\n",
      "WARWICK:\n",
      "Alas, my master, madam, he must have stand and plaw you:\n",
      "That I may pater to your highness that\n",
      "To Warwick thy city and them, and his shame.\n",
      "Thou art heaven the danger first of might well\n",
      "That I leave, to fight him for that been!\n",
      "And by and advance in a footh in courtey,\n",
      "And Ireland have for hi\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%run small_sample.py --out_dir=out-shakespeare-char --device=cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, it seems that most (possibly all) of the models we trained overfit. They all have a training loss that's significantly lower than the validation loss, indicating that we maybe should've stopped earlier. I think that it's difficult for this model to generate anything beyond meaningless strings of words because we're only encoding characters as tokens, rather than sub-words. Another conclusion to draw is that the embedding for a single character might not carry much meaning, and we might not even need a very high dimensional space to capture the different meanings of the characters. A large space might essentially learn one-hot encoding. We'd have to significantly improve the model by increasing context length (or just the size of the model in general), tokenizing subwords, and training on more data to get output that sounds like a real Shakespeare story. Right now, with this small model, we're only able to generate words that sort of sound like Shakespeare, but won't pass for real Shakespeare."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
